{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam10816/CS22M082_ASSIGNEMNT_1/blob/q2/March_111__neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hhgoo0AzGUZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c9a08e-6615-4db1-c7f4-76ce18454e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.13.11)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.16.0)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titles = [\"T-Shirt/Top\",\"Trouser\",\"Pullover\",\"Dress\",\"Sself.hirts\",\"Sandal\",\"Coat\",\"Sneaker\",\"Bag\",\"Ankle boot\"]"
      ],
      "metadata": {
        "id": "NGm9Aj9AFy8V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init(project = \"Assignment 1_new\" ,name = \"Question 1\")\n",
        "\n",
        "\n",
        "\n",
        "# fig,axs = plt.subplots(2,5,figsize=(20,6))\n",
        "# axs =axs.flatten()\n",
        "# images=[]\n",
        "# for i in range(0,10):\n",
        "#   index =random.choice(np.where(train_labels==i)[0])\n",
        "  \n",
        "#   axs[i].imshow(train_data[index],cmap=\"gray\")\n",
        "#   axs[i].set_title(titles[i])\n",
        "#   Img = wandb.Image(train_data[index],caption=[titles[i]])\n",
        "#   images.append(Img)\n",
        "# wandb.log({\"examples\":images})\n"
      ],
      "metadata": {
        "id": "CG7IszZv9655"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2p4rX01kwJw7"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_data = np.reshape(train_data,(len(train_data),train_data.shape[1]**2))\n",
        "test_data = np.reshape(test_data,(len(test_data),test_data.shape[1]**2))\n",
        "test_labels =np.reshape(test_labels,(1,len(test_data)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qviYKZc-UJTo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2/3**"
      ],
      "metadata": {
        "id": "dmMY0qTlUMea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qOS7o3xrG1Yn"
      },
      "outputs": [],
      "source": [
        "import torch.nn.init as init\n",
        "import torch\n",
        "from numpy.core.multiarray import MAY_SHARE_EXACT\n",
        "from re import M\n",
        "from matplotlib.lines import STEP_LOOKUP_MAP\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "train_data = np.reshape(train_data/255.0,(len(train_data),train_data.shape[1]**2))\n",
        "test_data = np.reshape(test_data/255.0,(len(test_data),test_data.shape[1]**2))\n",
        "test_labels =np.reshape(test_labels,(1,len(test_data)))\n",
        "\n",
        "\n",
        "class neural_network:\n",
        "\n",
        "  #it initializes W and b\n",
        "  def __init__(self):\n",
        "    \n",
        "    (train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "    train_data = np.reshape(train_data/255.0,(len(train_data),train_data.shape[1]**2))\n",
        "    self.test_data = np.reshape(test_data/255.0,(len(test_data),test_data.shape[1]**2))\n",
        "    self.test_labels =test_labels #np.reshape(test_labels,(1,len(test_data)))\n",
        "\n",
        "    l=int(train_data.shape[0]/100)*80\n",
        "    self.train_data=train_data[0:l]\n",
        "    self.train_label=train_labels[0:l]\n",
        "    self.validation_data = train_data[l:]\n",
        "    self.validation_label = train_labels[l:]\n",
        "    \n",
        "  #def train(self,epoch=10,hidden_layers=3,size_of_layer=32,weight_decay=0,learning_rate=0.01,optimizer=\"sgd\",batch_size=32,weight_init=\"random\",activation=\"sigmoid\"):\n",
        "\n",
        "  def train(self,weight_init=\"random\",hidden_layers=3,size_of_layer=32,activation=\"sigmoid\",optimizer=\"sgd\",learning_rate=0.01,epoch=10,batch_size=32,weight_decay=0.005,loss=\"cross_entropy\"):\n",
        "    \n",
        "    np.random.seed(42)\n",
        "    self.loss = loss\n",
        "    self.hi=[size_of_layer]*hidden_layers\n",
        "    self.activation =activation\n",
        "    if(weight_init==\"Xavier\"):\n",
        "      self.xav()\n",
        "    elif(weight_init==\"random\"):\n",
        "      self.rndm()\n",
        "   \n",
        "    if(optimizer==\"sgd\"):\n",
        "      self.sgd(step_size=learning_rate,batch_size =batch_size,epoch=epoch,reg=weight_decay)\n",
        "    elif(optimizer==\"momentum\"):\n",
        "      self.mbgd(step_size=learning_rate,batch_size =batch_size,epoch=epoch,beta=0.75,reg=weight_decay)\n",
        "    elif(optimizer==\"nesterov\"):\n",
        "      self.nagd(step_size=learning_rate,batch_size =batch_size,epoch=epoch,reg=weight_decay)\n",
        "    elif(optimizer==\"rmsprop\"):\n",
        "      self.rmsprop(step_size=learning_rate,batch_size =batch_size,epoch=epoch,reg=weight_decay)\n",
        "    elif(optimizer==\"adam\"):\n",
        "      self.adam(step_size=learning_rate,batch_size =batch_size,epoch=epoch,reg=weight_decay)\n",
        "    elif(optimizer==\"nadam\"):\n",
        "      self.nadam(step_size=learning_rate,batch_size =batch_size,epoch=epoch,reg=weight_decay)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  def xav(self):\n",
        "    l= self.train_data.shape[1]\n",
        "\n",
        "    self.W =[self.xavier_init(self.hi[0],l)] \n",
        "    self.b =[self.xavier_init(1,self.hi[0])]\n",
        "    for i in range(1,len(self.hi)) :\n",
        "      self.W.append(self.xavier_init(self.hi[i],self.hi[i-1]))\n",
        "      self.b.append(self.xavier_init(1,self.hi[i])) \n",
        "    self.W.append(self.xavier_init(10,self.hi[-1]))\n",
        "    \n",
        "    self.b.append(self.xavier_init(1,10))\n",
        "  \n",
        "\n",
        "  def rndm(self):\n",
        "    l= train_data.shape[1]\n",
        "\n",
        "    self.W =[np.random.randn(self.hi[0],l)] \n",
        "    self.b =[np.random.randn(1,self.hi[0])]\n",
        "    for i in range(1,len(self.hi)) :\n",
        "      self.W.append(np.random.randn(self.hi[i],self.hi[i-1]))\n",
        "      self.b.append(np.random.randn(1,self.hi[i])) \n",
        "    self.W.append(np.random.randn(10,self.hi[-1]))\n",
        "    \n",
        "    self.b.append(np.random.randn(1,10))\n",
        "\n",
        "  def relu(self,matrix):\n",
        "\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    matrix -- a NumPy array or matrix\n",
        "    Returns:\n",
        "    A NumPy array or matrix of the same shape as the input matrix,\n",
        "    with ReLU applied to each element.\n",
        "    \"\"\"\n",
        "    return np.maximum(matrix, 0) \n",
        "\n",
        "  def relu_derivative(self,matrix):\n",
        "    \n",
        "    # Create a copy of the input matrix and convert to float\n",
        "    derivative = np.array(matrix, dtype=np.float64)\n",
        "    \n",
        "    # Set negative values to 0\n",
        "    derivative[derivative < 0] = 0\n",
        "    \n",
        "    # Set positive values to 1\n",
        "    derivative[derivative > 0] = 1\n",
        "\n",
        "    return derivative\n",
        "\n",
        "  def tanh(self,matrix):\n",
        "    \n",
        "    # Avoid overflow by scaling inputs to the range [-100, 100]\n",
        "    x = np.clip(matrix, -100, 100)\n",
        "    \n",
        "    # Apply tanh element-wise\n",
        "    return np.tanh(x)\n",
        "\n",
        "  def tanh_derivative(self,matrix):\n",
        "   \n",
        "    # Avoid overflow by scaling inputs to the range [-100, 100]\n",
        "    x = np.clip(matrix, -100, 100)\n",
        "    \n",
        "    # Compute tanh element-wise\n",
        "    tanh_x = np.tanh(x)\n",
        "    \n",
        "    # Compute derivative of tanh element-wise\n",
        "    derivative = 1 - tanh_x**2\n",
        "    \n",
        "    \n",
        "    \n",
        "    return derivative\n",
        "\n",
        " \n",
        "  def WX_plus_B(self,W, X, b):\n",
        "    \n",
        "    result = np.dot(X, W.transpose())\n",
        "    row_count = result.shape[0]\n",
        "    \n",
        "    row_matrix_repeated = np.tile(b, (row_count, 1))\n",
        "    return result + row_matrix_repeated\n",
        "\n",
        "\n",
        "  def xavier_init(self,n, m):\n",
        "    # Calculate the Xavier initialization scale factor\n",
        "    xavier_scale = np.sqrt(2.0 / (n + m))\n",
        "\n",
        "    # Use numpy's random function to generate a matrix of shape (n, m)\n",
        "    matrix = np.random.randn(n, m) * xavier_scale\n",
        "\n",
        "    return matrix\n",
        "\n",
        "  #calculates sigmoid for matrix\n",
        "  def sigmoid(self,x):\n",
        "  \n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "  \n",
        "  def sum_columns(self,matrix):\n",
        "    if isinstance(matrix, np.ndarray):\n",
        "        # if matrix is a numpy array, convert it to a list\n",
        "        matrix = matrix.tolist()\n",
        "    \n",
        "    # sum the elements of each column and store in a list\n",
        "    column_sums = [sum(col) for col in zip(*matrix)]\n",
        "    \n",
        "    # convert the list to a 2D matrix of shape (1 x n)\n",
        "    row_matrix = np.array([column_sums])\n",
        "    \n",
        "    return row_matrix\n",
        "  #softmax for matrix\n",
        "  def softmax(self,x):\n",
        "    # Subtract the maximum value in each row from all the values in that row\n",
        "    # to prevent numerical instability from very large or very small values\n",
        "    # in the exponentials of the softmax function.\n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "  \n",
        "  def subtract_matrices(self,W, W_theta, step_size):\n",
        "    \"\"\"\n",
        "    Subtract the matrices in the second list from the matrices in the first list, after multiplying the matrices in the\n",
        "    second list by a step size.\n",
        "\n",
        "    Args:\n",
        "        first_list (list): A list of numpy arrays representing the first set of matrices.\n",
        "        second_list (list): A list of numpy arrays representing the second set of matrices.\n",
        "        step_size (float): The step size to multiply the second set of matrices by.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of numpy arrays representing the result of subtracting the second set of matrices from the first set\n",
        "        of matrices after multiplying the second set of matrices by the step size.\n",
        "    \"\"\"\n",
        "    result_list = []\n",
        "    for i in range(len(self.W)):\n",
        "        result = W[i] - step_size * (W_theta[i])\n",
        "        result_list.append(result)\n",
        "    return result_list\n",
        "\n",
        "  def sigmoid_derivative(self,matrix):\n",
        "    \"\"\"\n",
        "    Calculate the derivative of the sigmoid function on a 2D matrix.\n",
        "\n",
        "    Args:\n",
        "        matrix (numpy.ndarray): A numpy array representing the matrix.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A numpy array representing the result of calculating the sigmoid derivative on the matrix.\n",
        "    \"\"\"\n",
        "    shift = np.max(matrix, axis=1, keepdims=True)\n",
        "    exp_matrix = np.exp(matrix - shift)\n",
        "    sig = 1 / (1 + exp_matrix)\n",
        "    return sig * (1 - sig)\n",
        "  \n",
        "  \n",
        "\n",
        "  def mean_squared_error(self,y_hat, y):\n",
        "    n = y_hat.shape[0]  # number of samples\n",
        "    k = y.astype(int)  # convert y to integer type for indexing\n",
        "    y_k = np.zeros((n, 10))  # create a one-hot encoding of y\n",
        "    y_k[np.arange(n), k] = 1\n",
        "    \n",
        "    # Calculate mean squared error\n",
        "    mse = np.mean((y_hat - y_k)**2)\n",
        "    \n",
        "    return mse\n",
        "  \n",
        "  def mean_squared_error_derivative(self,y_hat, y):\n",
        "    n = y_hat.shape[0]  # number of samples\n",
        "    k = y.astype(int)  # convert y to integer type for indexing\n",
        "    y_k = np.zeros((n, 10))  # create a one-hot encoding of y\n",
        "    y_k[np.arange(n), k] = 1\n",
        "    \n",
        "    # Calculate derivative\n",
        "    dMSE_dy_hat = (2/n) * (y_hat - y_k)\n",
        "    \n",
        "    return dMSE_dy_hat\n",
        "  def cross_entropy_loss_derivative(self,y_hat, Y):\n",
        "    ey = np.zeros((y_hat.shape[0],y_hat.shape[1]))\n",
        "\n",
        "    for i in range(0,len(Y)):\n",
        "      ey[i][Y[i]]=1\n",
        "    \n",
        "    return (-(ey-y_hat))\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    A,H,y_hat =self.forward_pro(X_test,self.W,self.b)\n",
        "  \n",
        "    y_pred = np.argmax(y_hat, axis=1)\n",
        "    return y_pred\n",
        "  def forward_pro(self,X,W,b):\n",
        "    A=[]\n",
        "    H=[]\n",
        "    A.append(self.WX_plus_B(W[0],X,b[0])) # a0 = WoX +bo\n",
        "\n",
        "    for i in range(1,len(self.hi)):\n",
        "\n",
        "      H.append(self.activation_fun(A[-1])) # hi = g(ai)\n",
        "      #print(H[i-1])\n",
        "      A.append( self.WX_plus_B(W[i],H[-1],b[i])) # ai = WiX +bi\n",
        "\n",
        "    H.append(self.activation_fun(A[-1]))\n",
        "    A.append(self.WX_plus_B(W[-1],H[-1],b[-1]))\n",
        "    \n",
        "    y_hat = self.softmax((A[-1]))\n",
        "    \n",
        "    \n",
        "    \n",
        "    return A,H,y_hat\n",
        "\n",
        "  def back_prop(self,X,Y,A,H,y_hat):\n",
        "    W_theta , b_theta,H_theta,A_theta =[],[],[],[]\n",
        "    \n",
        "    \n",
        "    \n",
        "  \n",
        "  \n",
        "    L =len(A)\n",
        "    if(self.loss==\"cross_entropy\"):\n",
        "      A_theta.append(self.cross_entropy_loss_derivative(y_hat,Y))\n",
        "    if(self.loss==\"MSE\"):\n",
        "      A_theta.append(self.mean_squared_error_derivative(y_hat,Y))\n",
        "    \n",
        "    #-------------------------\n",
        "    for k in range(L-1,0,-1):\n",
        "      \n",
        "      W_theta.append((np.matmul(A_theta[-1].transpose(),H[k-1])+self.reg*self.W[k]) ) # athetak*h[k-1]\n",
        "      b_theta.append( self.sum_columns(A_theta[-1]))\n",
        "      H_theta.append(np.matmul(A_theta[-1],self.W[k]))\n",
        "  \n",
        "      A_theta.append(H_theta[-1]*self.activation_derivative(A[k-1]))\n",
        "\n",
        "    W_theta.append((np.matmul(A_theta[-1].transpose(),X)+self.reg*self.W[0]))\n",
        "    b_theta.append(self.sum_columns(A_theta[-1]))\n",
        "\n",
        "    W_theta.reverse()\n",
        "    b_theta.reverse()\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    return W_theta , b_theta\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  def accuracy(self, X_test, y_test):\n",
        "    \n",
        "    # Feed forward through the network\n",
        "    A,H,y_hat =self.forward_pro(X_test,self.W,self.b)\n",
        "    \n",
        "    \n",
        "    y_pred = np.argmax(y_hat, axis=1)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    acc = np.mean(y_pred == y_test)\n",
        "    # Calculate accuracy\n",
        "    \n",
        "\n",
        "    return acc\n",
        "\n",
        "  def cross_entropy(self,y_hat,Y):\n",
        "\n",
        "      epsilon = 1e-9\n",
        "      y_hat = np.maximum(y_hat, epsilon)\n",
        "      sum=0.0;\n",
        "      for i in range(0,len(Y)):\n",
        "        sum+=(-np.log2(y_hat[i][Y[i]]))\n",
        "      sum/= float(len(Y))\n",
        "      return sum\n",
        "  \n",
        "  def sgd(self,step_size,batch_size,epoch,reg=0.9):\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)\n",
        "      start_time = time.time()\n",
        "      for k in range(0,N):\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "          self.b =self.subtract_matrices(self.b,db,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,dW,step_size)\n",
        "          \n",
        "          \n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "      \n",
        "  def mbgd(self,step_size,batch_size,epoch,beta=0.9,reg=0):\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "\n",
        "    prev_ub , prev_uw =[],[]\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      prev_ub.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      prev_uw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "\n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)\n",
        "      start_time = time.time()\n",
        "      beta_t=beta\n",
        "      for k in range(0,N):\n",
        "          ub,uw = list(prev_ub),list(prev_uw)\n",
        "\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          for i in range(len(self.W)):\n",
        "            ub[i]= beta_t*prev_ub[i] + db[i]\n",
        "            uw[i]= beta_t*prev_uw[i] + dW[i]\n",
        "          self.b =self.subtract_matrices(self.b,ub,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,uw,step_size)\n",
        "\n",
        "          prev_ub , prev_uw = list(ub),list(uw)\n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "      \n",
        "  def nagd(self,step_size,batch_size,epoch,beta=0.9,reg=0):\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "\n",
        "    prev_ub , prev_uw =[],[]\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      prev_ub.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      prev_uw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "\n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "      beta_t=beta\n",
        "      step_size=rate/(e+1)\n",
        "      start_time = time.time()\n",
        "      for k in range(0,N):\n",
        "          ub,uw = list(prev_ub),list(prev_uw)\n",
        "          n_w ,n_b =self.subtract_matrices(self.W,prev_uw,beta_t),self.subtract_matrices(self.b,prev_ub,beta_t)\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          A,H,y_hat=self.forward_pro(minibatch,n_w,n_b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          for i in range(len(self.W)):\n",
        "            ub[i]= beta_t*prev_ub[i] + db[i]\n",
        "            uw[i]= beta_t*prev_uw[i] + dW[i]\n",
        "          self.b =self.subtract_matrices(self.b,ub,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,uw,step_size)\n",
        "\n",
        "          prev_ub , prev_uw = list(ub),list(uw)\n",
        "      \n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "\n",
        "  def rmsprop(self,step_size,batch_size,epoch,beta=0.9,reg=0,epsilon=1e-10):\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "\n",
        "    ub , uw =[],[]\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      ub.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      uw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "\n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)\n",
        "      start_time = time.time()\n",
        "      beta_t = beta\n",
        "      for k in range(0,N):\n",
        "         \n",
        "\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          for i in range(len(self.W)):\n",
        "            ub[i]= beta_t*ub[i] + (1-beta_t)*(db[i]**2)\n",
        "            uw[i]= beta_t*uw[i] + (1-beta_t)*(dW[i]**2)\n",
        "          \n",
        "          for i in range(len(self.W)):\n",
        "            result_b = self.b[i] - step_size*db[i]/(np.sqrt(ub[i])+epsilon)\n",
        "            result_w = self.W[i] - step_size*dW[i]/(np.sqrt(uw[i])+epsilon)\n",
        "            self.b[i]=result_b\n",
        "            self.W[i]=result_w\n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "      \n",
        "  \n",
        "  def adam(self,step_size,batch_size,epoch,beta1=0.9,beta2=0.999,reg=0,epsilon=1e-4):\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    rate = step_size\n",
        "    vb , vw =[],[]\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      vb.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      vw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "    mw=list(vw)\n",
        "    mb=list(vb)\n",
        "\n",
        "    \n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)\n",
        "      beta1_t,beta2_t = beta1,beta2\n",
        "      \n",
        "      for k in range(0,N):\n",
        "         \n",
        "\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          for i in range(len(self.W)):\n",
        "            mw[i]=  beta1_t*mw[i] + (1-beta1_t)*dW[i]\n",
        "            mb[i]=  beta1_t*mb[i] + (1-beta1_t)*db[i]\n",
        "            vb[i]= beta2_t*vb[i] + (1-beta2_t)*db[i]**2\n",
        "            vw[i]= beta2_t*vw[i] + (1-beta2_t)*dW[i]**2\n",
        "\n",
        "            mw_hat=mw[i]/(1-np.power(beta1_t,e+1))\n",
        "            mb_hat=mb[i]/(1-np.power(beta1_t,e+1))\n",
        "            vw_hat=vw[i]/(1-np.power(beta2_t,e+1))\n",
        "            vb_hat=vb[i]/(1-np.power(beta2_t,e+1))\n",
        "          \n",
        "            result_b = self.b[i] - step_size*mb_hat/(np.sqrt(vb_hat)+epsilon)\n",
        "            result_w = self.W[i] - step_size*mw_hat/(np.sqrt(vw_hat)+epsilon)\n",
        "            self.b[i]=result_b\n",
        "            self.W[i]=result_w\n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "  \n",
        "  def nadam(self,step_size,batch_size,epoch,beta1=0.9,beta2=0.999,reg=0,epsilon=1e-4):\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    rate = step_size\n",
        "    vb , vw =[],[]\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      vb.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      vw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "    mw=list(vw)\n",
        "    mb=list(vb)\n",
        "\n",
        "    \n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)\n",
        "      beta1_t,beta2_t = beta1,beta2\n",
        "      start_time = time.time()\n",
        "      for k in range(0,N):\n",
        "         \n",
        "\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          for i in range(len(self.W)):\n",
        "            mw[i]=  beta1_t*mw[i] + (1-beta1_t)*dW[i]\n",
        "            mb[i]=  beta1_t*mb[i] + (1-beta1_t)*db[i]\n",
        "            vb[i]= beta2_t*vb[i] + (1-beta2_t)*db[i]**2\n",
        "            vw[i]= beta2_t*vw[i] + (1-beta2_t)*dW[i]**2\n",
        "\n",
        "            mw_hat=mw[i]/(1-np.power(beta1_t,e+1))\n",
        "            mb_hat=mb[i]/(1-np.power(beta1_t,e+1))\n",
        "            vw_hat=vw[i]/(1-np.power(beta2_t,e+1))\n",
        "            vb_hat=vb[i]/(1-np.power(beta2_t,e+1))\n",
        "          \n",
        "            result_w = self.W[i] -(step_size/np.sqrt(vw_hat+epsilon))*(beta1_t*mw_hat+(1-beta1_t)*dW[i]/(1-beta1_t**(e+1)))\n",
        "            result_b = self.b[i] -(step_size/np.sqrt(vb_hat+epsilon))*(beta1_t*mb_hat+(1-beta1_t)*db[i]/(1-beta1_t**(e+1)))\n",
        "            self.b[i]=result_b\n",
        "            self.W[i]=result_w\n",
        "      \n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "\n",
        "\n",
        "  def activation_fun(self,matrix):\n",
        "      if(self.activation==\"sigmoid\"):\n",
        "        return self.sigmoid(matrix)\n",
        "      if(self.activation==\"tanh\"):\n",
        "        return self.tanh(matrix)\n",
        "      if(self.activation==\"ReLU\"):\n",
        "        return self.relu(matrix)\n",
        "  \n",
        "  def activation_derivative(self,matrix):\n",
        "      if(self.activation==\"sigmoid\"):\n",
        "        return self.sigmoid_derivative(matrix)\n",
        "      if(self.activation==\"tanh\"):\n",
        "        return self.tanh_derivative(matrix)\n",
        "      if(self.activation==\"ReLU\"):\n",
        "        return self.relu_derivative(matrix)\n",
        "\n",
        "     \n",
        "      \n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "81ZgkBC9_bGE",
        "outputId": "2fdb8a9d-7b39-4d44-f5b7-5e2d7725d81d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-27f55e1bdb9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ReLU\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nadam\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize_of_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"random\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# activation: reLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-cfa1ec811970>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, weight_init, hidden_layers, size_of_layer, activation, optimizer, learning_rate, epoch, batch_size, weight_decay, loss)\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"nadam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnadam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-cfa1ec811970>\u001b[0m in \u001b[0;36mnadam\u001b[0;34m(self, step_size, batch_size, epoch, beta1, beta2, reg, epsilon)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m       \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"validation_accuracy\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m       \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"validation_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidation_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"train_accuracy\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
          ]
        }
      ],
      "source": [
        "Net = neural_network()\n",
        "Net.train(activation=\"ReLU\",batch_size=16,epoch=10,hidden_layers=4,learning_rate=0.0001,optimizer=\"nadam\",size_of_layer=32,weight_init=\"random\",weight_decay=0.0005)\n",
        "\n",
        "\n",
        "# activation: reLU\n",
        "# wandb: \tbatch_size: 16\n",
        "# wandb: \tepoch: 10\n",
        "# wandb: \thidden_layers: 4\n",
        "# wandb: \tlearning_rate: 0.0001\n",
        "# wandb: \toptimizer: nadam\n",
        "# wandb: \tsize_of_layer: 32\n",
        "# wandb: \tweight_decay: 0.0005\n",
        "# wandb: \tweight_init: random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question.4**"
      ],
      "metadata": {
        "id": "57JWtdByTpyu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aysDESkXrvaz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c6e83c4-3dd8-40bf-c781-e6868908a37d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: gf9da7of\n",
            "Sweep URL: https://wandb.ai/vilgax/March-10/sweeps/gf9da7of\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7if6fi10 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_of_layer: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: Xavier\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_init.py\", line 1140, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_init.py\", line 215, in setup\n",
            "    tel.feature.set_init_tags = True\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
            "    self._run._telemetry_callback(self._obj)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 689, in _telemetry_callback\n",
            "    self._telemetry_flush()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 700, in _telemetry_flush\n",
            "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface_shared.py\", line 101, in _publish_telemetry\n",
            "    self._publish(rec)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n",
            "Exception in thread Thread-39:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_init.py\", line 1140, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_init.py\", line 215, in setup\n",
            "    tel.feature.set_init_tags = True\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
            "    self._run._telemetry_callback(self._obj)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 689, in _telemetry_callback\n",
            "    self._telemetry_flush()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 700, in _telemetry_flush\n",
            "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface_shared.py\", line 101, in _publish_telemetry\n",
            "    self._publish(rec)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/agents/pyagent.py\", line 298, in _run_job\n",
            "    self._function()\n",
            "  File \"<ipython-input-18-90874d91a32e>\", line 41, in train_nn\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_init.py\", line 1181, in init\n",
            "    raise Exception(\"problem\") from error_seen\n",
            "Exception: problem\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.9/threading.py\", line 917, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/agents/pyagent.py\", line 303, in _run_job\n",
            "    wandb.finish(exit_code=1)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 3669, in finish\n",
            "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 368, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 331, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 1843, in finish\n",
            "    return self._finish(exit_code, quiet)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 1851, in _finish\n",
            "    tel.feature.finish = True\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
            "    self._run._telemetry_callback(self._obj)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 689, in _telemetry_callback\n",
            "    self._telemetry_flush()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 700, in _telemetry_flush\n",
            "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface_shared.py\", line 101, in _publish_telemetry\n",
            "    self._publish(rec)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zfount1c with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_of_layer: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: Xavier\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sweep_config = {\n",
        "    \n",
        "\n",
        "    \"method\": 'random',\n",
        "    \"metric\":{\n",
        "        'name':'accuracy',\n",
        "        'goal':'maximize'\n",
        "    },\n",
        "    'parameters' :{\n",
        "        \"weight_init\" :{\"values\":[\"random\",\"Xavier\"]},\n",
        "        \"hidden_layers\": {\"values\": [ 3,4,5,6]},\n",
        "        \"size_of_layer\": {\"values\": [ 32, 64,128]},\n",
        "        \"activation\": {\"values\": [\"sigmoid\", \"ReLU\",\"tanh\"]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\",\"momentum\",\"nesterov\", \"adam\",\"rmsprop\",\"nadam\"]},\n",
        "        \"learning_rate\": {\"values\": [0.001,0.0001,0.00001]},\n",
        "        \"epoch\": {\"values\": [5,10]},\n",
        "        \"batch_size\": {\"values\": [16,32,64]},\n",
        "        \"weight_decay\": {\"values\": [0.0005, 0.005, 0.05]}\n",
        "    }\n",
        "    \n",
        "}\n",
        "\n",
        "def train_nn():\n",
        "\n",
        "    config_default={\n",
        "    'weight_init':\"random\",\n",
        "    'hidden_layers':3,\n",
        "    'size_of_layer':32,\n",
        "    'activation':\"sigmoid\",\n",
        "    'optimizer':\"sgd\",\n",
        "    'learning_rate':0.01,\n",
        "    'epoch':10,\n",
        "    'batch_size':32,\n",
        "    'weight_decay':0\n",
        "    }\n",
        "    \n",
        "   \n",
        "    wandb.init(config=config_default)\n",
        "    config = wandb.config\n",
        "    name='init_'+str(config.weight_init)+'_hl_'+str(config.hidden_layers)+\"_SL_\"+str(config.size_of_layer)+'_BS_'+str(config.batch_size)+\"_LR_\"+str(config.learning_rate)+'_AF_'+str(config.activation)+'_OPT_'+str(config.optimizer)+'_epoch_'+str(config.epoch)\n",
        "    wandb.init(name=name)\n",
        "    Net = neural_network()\n",
        "    Net.train(epoch=config.epoch, hidden_layers=config.hidden_layers, size_of_layer=config.size_of_layer, batch_size=config.batch_size, activation=config.activation, optimizer=config.optimizer, weight_init=config.weight_init, learning_rate=config.learning_rate, weight_decay=config.weight_decay)\n",
        "    accuracy = Net.accuracy(Net.test_data,Net.test_labels)\n",
        "    wandb.log({\"testing_accuracy\": accuracy})\n",
        "    \n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"March-10\",)\n",
        "wandb.agent(sweep_id, function = train_nn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AMNO9uX-Kx_D"
      },
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK3Re/byO3S6Dh3kRy+yNZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}