{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj/uCkIFoeFayhtUf3LeEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam10816/CS22M082_ASSIGNEMNT_1/blob/main/Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSqFsYsDKyD3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "7oFsLmLMK27m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape[1]**2"
      ],
      "metadata": {
        "id": "mfqrYUoXHGjN",
        "outputId": "9a12f891-e2e0-43ff-d67e-206e7c46be40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.reshape(train_data/255,(len(train_data),train_data.shape[1]**2))\n",
        "test_data = np.reshape(test_data/255,(len(test_data),test_data.shape[1]**2))\n",
        "\n"
      ],
      "metadata": {
        "id": "jjDB3SysGUIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJ8l5f3bHzRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[0]"
      ],
      "metadata": {
        "id": "fXs-JLsFHzS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialization(hi,batch_size):\n",
        " \n",
        "  l= train_data.shape[1]\n",
        "\n",
        "  W =[xavier_init(hi[0],l)] \n",
        "  b =[xavier_init(1,hi[0])]\n",
        "  A =[np.zeros((batch_size,hi[0]))]\n",
        "  H = [np.zeros((batch_size,hi[0]))]\n",
        "  for i in range(1,len(hi)) :\n",
        "    W.append(xavier_init(hi[i],hi[i-1]))\n",
        "    b.append(xavier_init(1,hi[i]))\n",
        "    A.append(np.zeros((batch_size,hi[i])))\n",
        "    H.append(np.zeros((batch_size,hi[i])))\n",
        "  W.append(xavier_init(10,hi[-1]))\n",
        "  A.append(np.zeros((batch_size,10)))\n",
        "  b.append(xavier_init(1,10))\n",
        "  return W , b ,A,H\n"
      ],
      "metadata": {
        "id": "rus16MTqK_X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xgxonj6cL7tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def forward_pro(X,W,b,A,H):\n",
        " \n",
        "      A[0]=WX_plus_B(W[0],X,b[0]) # a0 = WoX +bo\n",
        "\n",
        "      for i in range(1,len(hi)):\n",
        "  \n",
        "        H[i-1]=sigmoid(A[i-1]) # hi = g(ai)\n",
        "        #print(H[i-1])\n",
        "        A[i]= WX_plus_B(W[i],H[i-1],b[i]) # ai = WiX +bi\n",
        "\n",
        "      H[-1]=sigmoid(A[-2])\n",
        "      A[-1]=WX_plus_B(W[-1],H[-1],b[-1])\n",
        "      \n",
        "      y_hat = softmax((A[-1]))\n",
        "      print(y_hat.shape)\n",
        "      \n",
        "      \n",
        "      return A,H,y_hat\n",
        "\n",
        "def back_prop(X,Y,A,H,y_hat):\n",
        "\n",
        "       \n",
        "        W_theta , b_theta,H_theta,A_theta =W,b,H,A\n",
        "        #print(np.argmax(y_hat),lable)\n",
        "        ey = np.zeros((y_hat.shape[0],y_hat[1]))\n",
        "\n",
        "        for i in range(0,len(Y)):\n",
        "          ey[i][Y[i]]=1\n",
        "        \n",
        "        print(ey)\n",
        "       \n",
        "        L =len(A_theta)\n",
        "\n",
        "       \n",
        "        A_theta[L-1]=(-(ey-y_hat))\n",
        "        \n",
        "        #-------------------------\n",
        "        for k in range(L-1,0,-1):\n",
        "          \n",
        "          W_theta[k]=((np.matmul(A_theta[k].transpose(),H[k-1]))) # athetak*h[k-1]\n",
        "          b_theta[k]=A_theta[k]\n",
        "          H_theta[k-1]=np.matmul(A_theta[k],W[k])\n",
        "          \n",
        "          \n",
        "          \n",
        "          # print(\"-------g_z------------\")\n",
        "          # print((g_z*(1-g_z)))\n",
        "          A_theta[k-1]=H_theta[k-1]*(g_z*(1-g_z))\n",
        "\n",
        "          \n",
        "\n",
        "        x = np.reshape(image,(1,l));\n",
        "        \n",
        "        \n",
        "        W_theta[0] = ((np.matmul(A_theta[0].transpose(),x)))\n",
        "        b_theta[0]=A_theta[0]\n",
        "\n",
        "        # print(\"-------W-----\")\n",
        "        # for i in W_theta :\n",
        "        #   print(i)\n",
        "        # print(\"-------b-----\")\n",
        "        # for i in b_theta :\n",
        "        #   print(i)\n",
        "        # print(\"-------A-----\")\n",
        "        # for i in A_theta :\n",
        "        #   print(i)\n",
        "        # print(\"-------H-----\")\n",
        "        # for i in H_theta :\n",
        "        #   print(i)\n",
        "        \n",
        "        return W_theta , b_theta"
      ],
      "metadata": {
        "id": "NVBPzF-oMdoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A,H,y_hat=forward_pro(train_data[0:16],W,b,A,H)"
      ],
      "metadata": {
        "id": "YFPngCRDThPh",
        "outputId": "6abe318d-bdca-4cd3-acd2-cd4129e0eac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A,H,y_hat=forward_pro(train_data[0]/255,W,b,A,H)\n",
        "y=y_hat*0\n",
        "y[train_labels[0]]=1\n",
        "crossentropy_loss(y_hat,y)"
      ],
      "metadata": {
        "id": "QXRpptpkOhHD",
        "outputId": "7476c676-6f45-4844-e10e-508a67bd49fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.551060709813169"
            ]
          },
          "metadata": {},
          "execution_count": 332
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "      s = np.max(z)\n",
        "      e_x = np.exp(z - s)\n",
        "      div = np.sum(e_x)\n",
        "      return e_x / div\n",
        "    \n",
        "   \n",
        "\n",
        "# computes on 2d matrix\n",
        "def sigmoid(x):\n",
        "   \n",
        "    return np.where(x >= 0,\n",
        "                    1 / (1 + np.exp(-x)),\n",
        "                    np.exp(x) / (1 + np.exp(x)))\n",
        "\n",
        "\n",
        "\n",
        "def sigmoid_derivative(matrix):\n",
        "    \n",
        "    exp_matrix = np.exp(-matrix)\n",
        "    sigmoid = 1 / (1 + np.where(np.isinf(exp_matrix), np.zeros_like(exp_matrix), exp_matrix))\n",
        "    return sigmoid * (1 - sigmoid)\n",
        "\n"
      ],
      "metadata": {
        "id": "Dyxm1rdROuvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid_derivative(A[-1])"
      ],
      "metadata": {
        "id": "0UoIqDfLM0To",
        "outputId": "6c5451bb-6f23-4dce-8318-4dc171e2a863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.24230657, 0.20030412, 0.17356584, 0.21906345, 0.20934108,\n",
              "        0.24215349, 0.21053149, 0.10512802, 0.24038631, 0.17973665],\n",
              "       [0.24265949, 0.20089903, 0.17541156, 0.22361712, 0.20750907,\n",
              "        0.24093886, 0.20912062, 0.10435683, 0.24037988, 0.18421483],\n",
              "       [0.24065742, 0.19607907, 0.17702489, 0.22310925, 0.21298344,\n",
              "        0.23988938, 0.20910043, 0.1075794 , 0.24004795, 0.18738484],\n",
              "       [0.240832  , 0.19535781, 0.17644501, 0.22234523, 0.21167045,\n",
              "        0.24006589, 0.21038392, 0.10633788, 0.23987134, 0.18893054],\n",
              "       [0.24314646, 0.20057607, 0.17801636, 0.22157414, 0.21031044,\n",
              "        0.24034411, 0.20968088, 0.10913559, 0.23994782, 0.19232138],\n",
              "       [0.24018278, 0.19631591, 0.17224255, 0.22274676, 0.20738951,\n",
              "        0.24255458, 0.20840241, 0.10599805, 0.24005077, 0.18653575],\n",
              "       [0.24167209, 0.19971172, 0.17752873, 0.22130689, 0.21327764,\n",
              "        0.24241767, 0.2102344 , 0.11095171, 0.23852124, 0.18753216],\n",
              "       [0.24163566, 0.19655237, 0.17194539, 0.22228667, 0.20724315,\n",
              "        0.24269248, 0.2090621 , 0.11031326, 0.23819244, 0.18033964],\n",
              "       [0.23997605, 0.19531867, 0.18114693, 0.22102558, 0.21756193,\n",
              "        0.23898767, 0.21333771, 0.10947129, 0.23889468, 0.18881614],\n",
              "       [0.23866859, 0.1927166 , 0.17887784, 0.22199852, 0.21437681,\n",
              "        0.23917968, 0.2122555 , 0.1067479 , 0.24019162, 0.18495317],\n",
              "       [0.24160193, 0.20089395, 0.17454646, 0.22330974, 0.20809151,\n",
              "        0.24096218, 0.20888047, 0.10860904, 0.24003101, 0.18668541],\n",
              "       [0.24063445, 0.20076286, 0.17954319, 0.2223012 , 0.21132931,\n",
              "        0.2422714 , 0.21092474, 0.10724597, 0.24155622, 0.18580594],\n",
              "       [0.23988538, 0.19757267, 0.17419647, 0.22249445, 0.21294355,\n",
              "        0.24149224, 0.20963968, 0.10598598, 0.2416762 , 0.18862277],\n",
              "       [0.24047837, 0.19674639, 0.17773644, 0.22138768, 0.21455001,\n",
              "        0.24139488, 0.20979921, 0.11019072, 0.23932145, 0.18892101],\n",
              "       [0.24055524, 0.19688531, 0.1790813 , 0.22031401, 0.21467169,\n",
              "        0.23973848, 0.21128168, 0.11089798, 0.23898135, 0.18865997],\n",
              "       [0.24199996, 0.19948017, 0.17591098, 0.22164599, 0.20958255,\n",
              "        0.2413799 , 0.2107882 , 0.10819294, 0.24084316, 0.18006414]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def WX_plus_B(W, X, b):\n",
        "    \n",
        "    result = np.dot(X, W.transpose())\n",
        "    row_count = result.shape[0]\n",
        "    \n",
        "    row_matrix_repeated = np.tile(b, (row_count, 1))\n",
        "    return result + row_matrix_repeated"
      ],
      "metadata": {
        "id": "oOJji55UO0Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WX_plus_B(W[0],train_data[0:16],b[0]).shape"
      ],
      "metadata": {
        "id": "zkZkq7NJPGhF",
        "outputId": "56da0007-412f-454e-d347-48c77da52064",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jXboGSIWPAH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def substract(W,W_theta,step_size):\n",
        "      for i in range(0,len(W)):\n",
        "        W[i]-=(step_size*W_theta[i])\n",
        "      return dw"
      ],
      "metadata": {
        "id": "GnQ3sVKgS_Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def crossentropy_loss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Calculates the categorical cross-entropy loss between the predicted and actual labels.\n",
        "    \n",
        "    Args:\n",
        "    y_pred: (numpy array) predicted class probabilities, shape (num_samples, num_classes)\n",
        "    y_true: (numpy array) actual class labels, shape (num_samples, num_classes)\n",
        "    \n",
        "    Returns:\n",
        "    loss: (float) the categorical cross-entropy loss\n",
        "    \"\"\"\n",
        "    \n",
        "    # Calculate the cross-entropy loss for each sample\n",
        "    sample_losses = -np.sum(y_true * np.log(y_pred))\n",
        "    \n",
        "    # Average the losses over all samples\n",
        "    loss = np.mean(sample_losses)\n",
        "    \n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "ItoXWfbKUie1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRIbcvJYUnMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def xavier_init(input_size, output_size):\n",
        "    \"\"\"\n",
        "    Initialize weights using Xavier initialization.\n",
        "\n",
        "    Parameters:\n",
        "    input_size (int): number of input units.\n",
        "    output_size (int): number of output units.\n",
        "\n",
        "    Returns:\n",
        "    weights (ndarray): array of shape (input_size, output_size) containing the initialized weights.\n",
        "    \"\"\"\n",
        "    # Calculate the variance of the weights\n",
        "    variance = 2.0 / (input_size + output_size)\n",
        "\n",
        "    # Calculate the standard deviation of the weights\n",
        "    standard_deviation = np.sqrt(variance)\n",
        "\n",
        "    # Generate random weights from a normal distribution with mean 0 and standard deviation standard_deviation\n",
        "    weights = np.random.normal(loc=0, scale=standard_deviation, size=(input_size, output_size))\n",
        "\n",
        "    return weights\n"
      ],
      "metadata": {
        "id": "bNWjjzDrPFh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xavier_init(1,50)"
      ],
      "metadata": {
        "id": "gMv5-V-OGC0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hi=[50,50]\n",
        "step_size=0.0001\n",
        "epoch =2\n",
        "batch_size =10000\n",
        "W,b,A,H =initialization(hi,batch_size)\n",
        "print(\"-------W-----\")\n",
        "for i in W :\n",
        "  print(i.shape)\n",
        "print(\"-------b-----\")\n",
        "for i in b :\n",
        "  print(i.shape)\n",
        "print(\"-------A-----\")\n",
        "for i in A :\n",
        "  print(i.shape)\n",
        "print(\"-------H-----\")\n",
        "for i in H :\n",
        "  print(i.shape)\n",
        "# A,H,y_hat = forward_pro(train_data[0]/255,W,b,A,H)\n",
        "# W_theta,b_theta=back_prop(train_data[0]/255,train_labels[0],A,H,y_hat)\n",
        "# for i in range(0,len(hi)+1):\n",
        "#       #print(\"updating...\")\n",
        "#       b[i] -=(step_size*b_theta[i])\n",
        "#       W[i] -=(step_size*W_theta[i])\n",
        "# accuracy(test_data/255,test_labels,W,b,A,H)"
      ],
      "metadata": {
        "id": "bEhqa7IGrFlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for e in range(10):\n",
        "\n",
        "  for i in range(len(train_data)):\n",
        "\n",
        "    A,H,y_hat = forward_pro(train_data[i]/255,W,b,A,H)\n",
        "    W_theta,b_theta=back_prop(train_data[i]/255,train_labels[i],A,H,y_hat)\n",
        "\n",
        "    \n",
        "    print(\"Loss :- \",end=\" \")\n",
        "    y=y_hat*0\n",
        "    y[train_labels[i]]=1\n",
        "    print(crossentropy_loss(y_hat,y))\n",
        "\n",
        "    \n",
        "    for i in range(0,len(hi)+1):\n",
        "      #print(\"updating...\")\n",
        "      b[i] -=(step_size*b_theta[i])\n",
        "      W[i] -=(step_size*W_theta[i])\n",
        "      "
      ],
      "metadata": {
        "id": "jjHRS1eGLc-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        \n",
        "        # Initialize the weights and biases for the hidden layers\n",
        "        for i in range(len(hidden_sizes)):\n",
        "            if i == 0:\n",
        "                w = np.random.randn(input_size, hidden_sizes[i])\n",
        "            else:\n",
        "                w = np.random.randn(hidden_sizes[i-1], hidden_sizes[i])\n",
        "            b = np.zeros((1, hidden_sizes[i]))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "        \n",
        "        # Initialize the weights and biases for the output layer\n",
        "        w = np.random.randn(hidden_sizes[-1], output_size)\n",
        "        b = np.zeros((1, output_size))\n",
        "        self.weights.append(w)\n",
        "        self.biases.append(b)\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    def sigmoid_prime(self, z):\n",
        "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
        "        \n",
        "    def forward(self, X):\n",
        "        a = X\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
        "            if i == len(self.weights) - 1:\n",
        "                a = self.softmax(z)\n",
        "            else:\n",
        "                a = self.sigmoid(z)\n",
        "        return a\n",
        "        \n",
        "    def softmax(self, z):\n",
        "        e = np.exp(z - np.max(z))\n",
        "        return e / np.sum(e, axis=1, keepdims=True)\n",
        "        \n",
        "    def backward(self, X, y, output):\n",
        "        dW = [0] * len(self.weights)\n",
        "        db = [0] * len(self.biases)\n",
        "        delta = output - y\n",
        "        \n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            if i == len(self.weights) - 1:\n",
        "                dW[i] = np.dot(self.activations[i-1].T, delta)\n",
        "            elif i == 0:\n",
        "                dW[i] = np.dot(X.T, self.deltas[i])\n",
        "            else:\n",
        "                dW[i] = np.dot(self.activations[i-1].T, self.deltas[i])\n",
        "            db[i] = np.sum(delta, axis=0, keepdims=True)\n",
        "            if i > 0:\n",
        "                self.deltas[i-1] = np.dot(delta, self.weights[i].T) * self.sigmoid_prime(self.z[i-1])\n",
        "                delta = self.deltas[i-1]\n",
        "        \n",
        "        return dW, db\n",
        "        \n",
        "    def train(self, X_train, y_train, learning_rate, epochs, batch_size):\n",
        "        for i in range(epochs):\n",
        "            # Shuffle the training data\n",
        "            indices = np.random.permutation(len(X_train))\n",
        "            X_train = X_train[indices]\n",
        "            y_train = y_train[indices]\n",
        "            \n",
        "            # Split the data into batches\n",
        "            for j in range(0, len(X_train), batch_size):\n",
        "                X_batch = X_train[j:j+batch_size]\n",
        "                y_batch = y_train[j:j+batch_size]\n",
        "                \n",
        "                # Forward pass\n",
        "                self.activations = []\n",
        "                self.z = []\n",
        "                a = X_batch\n",
        "                for k in range(len(self.weights)):\n",
        "                    z = np.dot(a, self.weights[k])\n"
      ],
      "metadata": {
        "id": "hG3xyDNGcqb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3teaF_LRcy-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(b)\n",
        "print()\n",
        "start_time = time.time()\n",
        "A,H,y_hat = forward_pro(train_data[i],W,b,A,H)\n",
        "W_theta,b_theta=back_prop(train_data[i],train_labels[i],A,H,y_hat)\n",
        "\n",
        "for i in range(0,len(hi)+1):\n",
        "      #print(\"updating...\")\n",
        "      b[i] -=(step_size*b_theta[i])\n",
        "      W[i] -=(step_size*W_theta[i])\n",
        "print(b_theta)\n",
        "\n",
        "equal_arrays = comparison\n",
        "print(equal_arrays)\n",
        "print(\"time required for 60000 image \" , (time.time()-start_time)*60000,\" sec\")"
      ],
      "metadata": {
        "id": "XOKdbDVdp-U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=train_data\n",
        "test_data = test_data"
      ],
      "metadata": {
        "id": "oo8OEnhOnyss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(step_size,batch_size,epoch,W,b,A,H):\n",
        "    N = int(len(train_data)/batch_size)\n",
        "\n",
        "    for e in range(0,epoch):\n",
        "      start_time = time.time()\n",
        "      for k in range(0,N):\n",
        "          minibatch = train_data[k*batch_size:min(k*batch_size+batch_size,len(train_data))]\n",
        "          minibatch_lable=train_labels[k*batch_size:min(k*batch_size+batch_size,len(train_data))]\n",
        "          \n",
        "          A,H,y_hat=forward_pro(minibatch[0],W,b,A,H)\n",
        "          dw,db = back_prop(minibatch[0],minibatch_lable[0],A,H,y_hat)\n",
        "\n",
        "          # for i in range(1,len(minibatch)):\n",
        "          #   y_hat=forward_pro(minibatch[i],W,b,A,H)\n",
        "          #   w_theta,b_theta = back_prop(minibatch[i],minibatch_lable[i],A,H,y_hat)\n",
        "          #   for i in range(0,len(hi)+1):\n",
        "          #     dw[i] +=w_theta[i]\n",
        "          #     db[i] +=b_theta[i]\n",
        "          \n",
        "          # for i in range(0,len(hi)+1):\n",
        "          #   b[i] -=(step_size*db[i])\n",
        "          #   W[i] -=(step_size*dw[i])\n",
        "          \n",
        "      print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "MjlywxuAW5Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd(0.0001,32,3,W,b,A,H)"
      ],
      "metadata": {
        "id": "frHsL7YoVDDE",
        "outputId": "edc44c40-92dd-4c18-872c-dc95f54bbc44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 2.065664291381836 seconds ---\n",
            "--- 2.052457094192505 seconds ---\n",
            "--- 2.011143922805786 seconds ---\n"
          ]
        }
      ]
    }
  ]
}