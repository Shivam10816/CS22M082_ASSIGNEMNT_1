{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam10816/CS22M082_ASSIGNEMNT_1/blob/main/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hhgoo0AzGUZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b9f6e46-a861-4fb6-c8bd-8c8dcc1ba302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=b3e687932d84680837c0e4b550b42db308af096cf8d9985cc159231fb0c6f161\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init(project = \"Assignment 1_new\" ,name = \"Question 1\")\n",
        "# fig,axs = plt.subplots(2,5,figsize=(20,6))\n",
        "# axs =axs.flatten()\n",
        "# images=[]\n",
        "# for i in range(0,10):\n",
        "#   index =random.choice(np.where(train_labels==i)[0])\n",
        "  \n",
        "#   axs[i].imshow(train_data[index],cmap=\"gray\")\n",
        "#   axs[i].set_title(titles[i])\n",
        "#   Img = wandb.Image(train_data[index],caption=[titles[i]])\n",
        "#   images.append(Img)\n",
        "# wandb.log({\"examples\":images})\n"
      ],
      "metadata": {
        "id": "CG7IszZv9655"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2/3**"
      ],
      "metadata": {
        "id": "dmMY0qTlUMea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qOS7o3xrG1Yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2cccb5e-a77a-4674-f8c1-95d1bd630418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from numpy.core.multiarray import ndarray\n",
        "\n",
        "import wandb\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "#---------------------------import fashion mnist data---------------------------\n",
        "\n",
        "class neural_network:\n",
        "\n",
        "  \n",
        "  #-------------------------Constructor to take train_data and test_data--------\n",
        "  def __init__(self,train_data,train_labels,test_data,test_labels):\n",
        "    \n",
        "    #-----------------------Normalize the data----------------------------------\n",
        "\n",
        "    train_data = np.reshape(train_data/255.0,(len(train_data),train_data.shape[1]**2))\n",
        "    self.test_data = np.reshape(test_data/255.0,(len(test_data),test_data.shape[1]**2))\n",
        "    self.test_labels =test_labels \n",
        "\n",
        "    #-----------------------Split data in train/validation set(90:10)-----------\n",
        "\n",
        "    l=int(train_data.shape[0]/100)*90\n",
        "    self.train_data=train_data[0:l]\n",
        "    self.train_label=train_labels[0:l]\n",
        "    self.validation_data = train_data[l:]\n",
        "    self.validation_label = train_labels[l:]\n",
        "   \n",
        "    \n",
        "  #------Train function to fit neural network with different parameter----------\n",
        "\n",
        "  def train(self,weight_init=\"random\",hidden_layers=3,size_of_layer=32,activation=\"sigmoid\",optimizer=\"sgd\",learning_rate=0.01,epoch=10,batch_size=32,weight_decay=0.005,loss=\"cross_entropy\"):\n",
        "    \n",
        "\n",
        "    #-----------------------Weight Initialization-------------------------------\n",
        "    np.random.seed(42)\n",
        "    self.loss = loss\n",
        "    self.hi=[size_of_layer]*hidden_layers\n",
        "    self.activation =activation\n",
        "    if(weight_init==\"Xavier\"):\n",
        "      self.xav()\n",
        "    elif(weight_init==\"random\"):\n",
        "      self.rndm()\n",
        "\n",
        "    #-------------------------------------Optimizer-----------------------------\n",
        "    if(optimizer==\"sgd\"):\n",
        "      self.sgd(step_size=learning_rate,batch_size =batch_size,epoch=epoch,reg=weight_decay)\n",
        "    elif(optimizer==\"momentum\"):\n",
        "      self.mbgd(step_size=learning_rate,batch_size =batch_size,epoch=epoch,beta=0.75,reg=weight_decay)\n",
        "    elif(optimizer==\"nesterov\"):\n",
        "      self.nagd(step_size=learning_rate,batch_size =batch_size,epoch=epoch,reg=weight_decay)\n",
        "    elif(optimizer==\"rmsprop\"):\n",
        "      self.rmsprop(step_size=learning_rate,batch_size =batch_size,epoch=epoch,reg=weight_decay)\n",
        "    elif(optimizer==\"adam\"):\n",
        "      self.adam(step_size=learning_rate,batch_size =batch_size,epoch=epoch,reg=weight_decay)\n",
        "    elif(optimizer==\"nadam\"):\n",
        "      self.nadam(step_size=learning_rate,batch_size =batch_size,epoch=epoch,reg=weight_decay)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "  #------------------------------Xavier Initialization(W & b)-------------------\n",
        "\n",
        "  def xav(self):\n",
        "    l= self.train_data.shape[1]\n",
        "\n",
        "    self.W =[self.xavier_init(self.hi[0],l)] \n",
        "    self.b =[self.xavier_init(1,self.hi[0])]\n",
        "    for i in range(1,len(self.hi)) :\n",
        "      self.W.append(self.xavier_init(self.hi[i],self.hi[i-1]))\n",
        "      self.b.append(self.xavier_init(1,self.hi[i])) \n",
        "    self.W.append(self.xavier_init(10,self.hi[-1]))\n",
        "    \n",
        "    self.b.append(self.xavier_init(1,10))\n",
        "  \n",
        "\n",
        "  #-----------------xavier_init_function(Return Matrix of (n,m))----------------\n",
        "\n",
        "  def xavier_init(self,n, m):\n",
        "    # Calculate the Xavier initialization scale factor\n",
        "    xavier_scale = np.sqrt(2.0 / (n + m))\n",
        "\n",
        "    # Use numpy's random function to generate a matrix of shape (n, m)\n",
        "    matrix = np.random.randn(n, m) * xavier_scale\n",
        "\n",
        "    return matrix\n",
        "\n",
        "  #-----------------------------Random Initialization(W & b)--------------------\n",
        "\n",
        "  def rndm(self):\n",
        "    l= self.train_data.shape[1]\n",
        "\n",
        "    self.W =[np.random.randn(self.hi[0],l)] \n",
        "    self.b =[np.random.randn(1,self.hi[0])]\n",
        "    for i in range(1,len(self.hi)) :\n",
        "      self.W.append(np.random.randn(self.hi[i],self.hi[i-1]))\n",
        "      self.b.append(np.random.randn(1,self.hi[i])) \n",
        "    self.W.append(np.random.randn(10,self.hi[-1]))\n",
        "    \n",
        "    self.b.append(np.random.randn(1,10))\n",
        "\n",
        "  #--------------------------Relu Activation function---------------------------\n",
        "\n",
        "  def relu(self,matrix):\n",
        "    return np.maximum(matrix, 0) \n",
        "\n",
        "  #--------------------------Relu Activation Derivation function---------------\n",
        "\n",
        "  def relu_derivative(self,matrix):\n",
        "    \n",
        "    # Create a copy of the input matrix and convert to float\n",
        "    derivative = np.array(matrix, dtype=np.float64)\n",
        "    \n",
        "    # Set negative values to 0\n",
        "    derivative[derivative < 0] = 0\n",
        "    \n",
        "    # Set positive values to 1\n",
        "    derivative[derivative > 0] = 1\n",
        "\n",
        "    return derivative\n",
        "\n",
        "  #--------------------------Tanh Activation function---------------------------\n",
        "\n",
        "  def tanh(self,matrix):\n",
        "    \n",
        "    # Avoid overflow by scaling inputs to the range [-100, 100]\n",
        "    x = np.clip(matrix, -100, 100)\n",
        "    \n",
        "    # Apply tanh element-wise\n",
        "    return np.tanh(x)\n",
        "\n",
        "  #-------------------------Tanh Activation Derivative function-----------------\n",
        "  def tanh_derivative(self,matrix):\n",
        "   \n",
        "    # Avoid overflow by scaling inputs to the range [-100, 100]\n",
        "    x = np.clip(matrix, -100, 100)\n",
        "    \n",
        "    # Compute tanh element-wise\n",
        "    tanh_x = np.tanh(x)\n",
        "    \n",
        "    # Compute derivative of tanh element-wise\n",
        "    derivative = 1 - tanh_x**2\n",
        "    \n",
        "    \n",
        "    \n",
        "    return derivative\n",
        "\n",
        "  #-------------------------calculates sigmoid for matrix-----------------------\n",
        "\n",
        "  def sigmoid(self,x):\n",
        "  \n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "  #------------------------sigmoid_derivative_function--------------------------\n",
        "\n",
        "  def sigmoid_derivative(self,matrix):\n",
        "   \n",
        "    shift = np.max(matrix, axis=1, keepdims=True)\n",
        "    exp_matrix = np.exp(matrix - shift)\n",
        "    sig = 1 / (1 + exp_matrix)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "  #-------------------------WX_plus_B function----------------------------------\n",
        "\n",
        "  def WX_plus_B(self,W, X, b):\n",
        "    \n",
        "    #matrix multiply\n",
        "    result = np.dot(X, W.transpose())\n",
        "\n",
        "    \n",
        "    #make b of shape result\n",
        "    row_count = result.shape[0]\n",
        "    row_matrix_repeated = np.tile(b, (row_count, 1))\n",
        "\n",
        "    return result + row_matrix_repeated\n",
        "\n",
        "  #----------------------------sum each element of column-----------------------\n",
        "\n",
        "  def sum_columns(self,matrix):\n",
        "    if isinstance(matrix, np.ndarray):\n",
        "        # if matrix is a numpy array, convert it to a list\n",
        "        matrix = matrix.tolist()\n",
        "    \n",
        "    # sum the elements of each column and store in a list\n",
        "    column_sums = [sum(col) for col in zip(*matrix)]\n",
        "    \n",
        "    # convert the list to a 2D matrix of shape (1 x n)\n",
        "    row_matrix = np.array([column_sums])\n",
        "    \n",
        "    return row_matrix\n",
        "\n",
        "  #----------------softmax function for matrix----------------------------------\n",
        "\n",
        "  def softmax(self,x):\n",
        "    \n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "  \n",
        "  #-----------Substract Matrix (W -step_size*W_theta)---------------------------\n",
        "\n",
        "  def subtract_matrices(self,W, W_theta, step_size):\n",
        "    \n",
        "    result_list = []\n",
        "    for i in range(len(self.W)):\n",
        "        result = W[i] - step_size * (W_theta[i])\n",
        "        result_list.append(result)\n",
        "    return result_list\n",
        "\n",
        "  #------------------Mean square error function---------------------------------\n",
        "  def mean_squared_error(self,y_hat, y):\n",
        "    n = y_hat.shape[0]  # number of samples\n",
        "    k = y.astype(int)  # convert y to integer type for indexing\n",
        "    y_k = np.zeros((n, 10))  # create a one-hot encoding of y\n",
        "    y_k[np.arange(n), k] = 1\n",
        "    \n",
        "    # Calculate mean squared error\n",
        "    mse = np.mean((y_hat - y_k)**2)\n",
        "    \n",
        "    return mse\n",
        "  \n",
        "  #-------------------Mean square error Derivative function---------------------\n",
        "\n",
        "  def mean_squared_error_derivative(self,y_hat, y):\n",
        "    n = y_hat.shape[0]  # number of samples\n",
        "    k = y.astype(int)  # convert y to integer type for indexing\n",
        "    y_k = np.zeros((n, 10))  # create a one-hot encoding of y\n",
        "    y_k[np.arange(n), k] = 1\n",
        "    \n",
        "    # Calculate derivative\n",
        "    dMSE_dy_hat = (2/n) * (y_hat - y_k)\n",
        "    \n",
        "    return dMSE_dy_hat\n",
        "  \n",
        "  #--------------------cross_entropy_loss derivative----------------------------\n",
        "\n",
        "  def cross_entropy_loss_derivative(self,y_hat, Y):\n",
        "    ey = np.zeros((y_hat.shape[0],y_hat.shape[1]))\n",
        "\n",
        "    for i in range(0,len(Y)):\n",
        "      ey[i][Y[i]]=1\n",
        "    \n",
        "    return (-(ey-y_hat))\n",
        "\n",
        "  #---------------------Predict labels for Input Data---------------------------\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    A,H,y_hat =self.forward_pro(X_test,self.W,self.b)\n",
        "  \n",
        "    y_pred = np.argmax(y_hat, axis=1)\n",
        "    return y_pred\n",
        "  \n",
        "  #----------------------Forward Propogation Code-------------------------------\n",
        "\n",
        "  def forward_pro(self,X,W,b):\n",
        "\n",
        "    \n",
        "    A=[]\n",
        "    H=[]\n",
        "    A.append(self.WX_plus_B(W[0],X,b[0])) # a0 = WoX +bo\n",
        "    \n",
        "    for i in range(1,len(self.hi)):\n",
        "\n",
        "      H.append(self.activation_fun(A[-1])) # hi = g(ai)\n",
        "     \n",
        "      A.append( self.WX_plus_B(W[i],H[-1],b[i])) # ai = WiX +bi\n",
        "\n",
        "    H.append(self.activation_fun(A[-1]))         #hi  = g(ai)\n",
        "    A.append(self.WX_plus_B(W[-1],H[-1],b[-1]))  # ai = WiX +bi\n",
        "    \n",
        "    #--------apply softmax function for final probbilities----------------------\n",
        "    y_hat = self.softmax((A[-1]))\n",
        "\n",
        "    return A,H,y_hat\n",
        "  \n",
        "  #----------------Back Proppogation function-----------------------------------\n",
        "\n",
        "  def back_prop(self,X,Y,A,H,y_hat):\n",
        "\n",
        "\n",
        "    W_theta , b_theta,H_theta,A_theta =[],[],[],[]\n",
        "    \n",
        "    L =len(A)\n",
        "\n",
        "    #------------------------Check if loss is cross_entropy---------------------\n",
        "    if(self.loss==\"cross_entropy\"):\n",
        "      A_theta.append(self.cross_entropy_loss_derivative(y_hat,Y))\n",
        "    if(self.loss==\"MSE\"):\n",
        "      A_theta.append(self.mean_squared_error_derivative(y_hat,Y))\n",
        "    \n",
        "    #-------------------------W_theta , b_theta calculation---------------------\n",
        "\n",
        "    for k in range(L-1,0,-1):\n",
        "      \n",
        "      W_theta.append((np.matmul(A_theta[-1].transpose(),H[k-1])+self.reg*self.W[k]) ) # athetak*h[k-1]\n",
        "      b_theta.append( self.sum_columns(A_theta[-1]))                                \n",
        "      H_theta.append(np.matmul(A_theta[-1],self.W[k]))\n",
        "  \n",
        "      A_theta.append(H_theta[-1]*self.activation_derivative(A[k-1]))\n",
        "\n",
        "    W_theta.append((np.matmul(A_theta[-1].transpose(),X)+self.reg*self.W[0]))\n",
        "    b_theta.append(self.sum_columns(A_theta[-1]))\n",
        "\n",
        "    W_theta.reverse()\n",
        "    b_theta.reverse()\n",
        "\n",
        "    return W_theta , b_theta\n",
        "\n",
        "  #-------------------------Accuracy Calculation--------------------------------\n",
        "\n",
        "  def accuracy(self, X_test, y_test):\n",
        "    \n",
        "    # Feed forward through the network\n",
        "    A,H,y_hat =self.forward_pro(X_test,self.W,self.b)\n",
        "    \n",
        "    y_pred = np.argmax(y_hat, axis=1)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    acc = np.mean(y_pred == y_test)\n",
        "    # Calculate accuracy\n",
        "  \n",
        "    return acc\n",
        "\n",
        "  #---------------------------Cross Entropy Function----------------------------\n",
        "  def cross_entropy(self,y_hat,Y):\n",
        "\n",
        "      epsilon = 1e-9\n",
        "      y_hat = np.maximum(y_hat, epsilon) # to avoid invalid divide\n",
        "\n",
        "      sum=0.0;\n",
        "      for i in range(0,len(Y)):\n",
        "        sum+=(-np.log2(y_hat[i][Y[i]]))\n",
        "      sum/= float(len(Y))\n",
        "\n",
        "      return sum\n",
        "  \n",
        "  #-------------------------Stochastic Gradient Descent-------------------------\n",
        "\n",
        "  def sgd(self,step_size,batch_size,epoch,reg=0.9):\n",
        "\n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    rate = step_size\n",
        "\n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)        # Learning Rate Updation\n",
        "      \n",
        "      for k in range(0,N):\n",
        "\n",
        "          # -------------------Create  Batch of particular size-----------------\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #---------------Compute A,H,y_hat-------------------------------------\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "\n",
        "          #---------------Compute dW ,dw using back_propogation-----------------\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          #-------------------------Update W and b------------------------------\n",
        "          self.b =self.subtract_matrices(self.b,db,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,dW,step_size)\n",
        "          \n",
        "      #Calculate validation Loss,validation Accuracy , Training_loss , Training Accuracy    \n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      #-------Update values To Wandb------\n",
        "\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "\n",
        "  #-------------------------Momentum based gradient descent---------------------\n",
        "\n",
        "  def mbgd(self,step_size,batch_size,epoch,beta=0.9,reg=0.005):\n",
        "\n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "\n",
        "    prev_ub , prev_uw =[],[]\n",
        "\n",
        "    #---------------Initilalize All matrix with 0-------------------------------\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      prev_ub.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      prev_uw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "    \n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)\n",
        "      start_time = time.time()\n",
        "      beta_t=beta\n",
        "      for k in range(0,N):\n",
        "          ub,uw = list(prev_ub),list(prev_uw)\n",
        "\n",
        "          #------------------------Create Minibatch data------------------------\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #--------calculate dW ,db using forward and backword propogation------\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          #-----------------------weight update---------------------------------\n",
        "          for i in range(len(self.W)):\n",
        "            ub[i]= beta_t*prev_ub[i] + db[i]\n",
        "            uw[i]= beta_t*prev_uw[i] + dW[i]\n",
        "          self.b =self.subtract_matrices(self.b,ub,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,uw,step_size)\n",
        "\n",
        "          prev_ub , prev_uw = list(ub),list(uw)\n",
        "      #Calculate val_accuracy ,val_loss ,training accuracy ,training loss\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      #---------Calculate perticular loss----------------\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      \n",
        "      #------------------------Update to Wandb----------------------------------\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "      \n",
        "  def nagd(self,step_size,batch_size,epoch,beta=0.9,reg=0.005):\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "\n",
        "    prev_ub , prev_uw =[],[]\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      prev_ub.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      prev_uw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "\n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "      beta_t=beta\n",
        "      step_size=rate/(e+1)\n",
        "      start_time = time.time()\n",
        "      for k in range(0,N):\n",
        "          ub,uw = list(prev_ub),list(prev_uw)\n",
        "          n_w ,n_b =self.subtract_matrices(self.W,prev_uw,beta_t),self.subtract_matrices(self.b,prev_ub,beta_t)\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          A,H,y_hat=self.forward_pro(minibatch,n_w,n_b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          for i in range(len(self.W)):\n",
        "            ub[i]= beta_t*prev_ub[i] + db[i]\n",
        "            uw[i]= beta_t*prev_uw[i] + dW[i]\n",
        "          self.b =self.subtract_matrices(self.b,ub,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,uw,step_size)\n",
        "\n",
        "          prev_ub , prev_uw = list(ub),list(uw)\n",
        "      \n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "\n",
        "  def rmsprop(self,step_size,batch_size,epoch,beta=0.9,reg=0.005,epsilon=1e-10):\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "\n",
        "    ub , uw =[],[]\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      ub.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      uw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "\n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)\n",
        "      start_time = time.time()\n",
        "      beta_t = beta\n",
        "      for k in range(0,N):\n",
        "         \n",
        "\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          for i in range(len(self.W)):\n",
        "            ub[i]= beta_t*ub[i] + (1-beta_t)*(db[i]**2)\n",
        "            uw[i]= beta_t*uw[i] + (1-beta_t)*(dW[i]**2)\n",
        "          \n",
        "          for i in range(len(self.W)):\n",
        "            result_b = self.b[i] - step_size*db[i]/(np.sqrt(ub[i])+epsilon)\n",
        "            result_w = self.W[i] - step_size*dW[i]/(np.sqrt(uw[i])+epsilon)\n",
        "            self.b[i]=result_b\n",
        "            self.W[i]=result_w\n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "      \n",
        "  \n",
        "  def adam(self,step_size,batch_size,epoch,beta1=0.9,beta2=0.999,reg=0.005,epsilon=1e-4):\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    rate = step_size\n",
        "    vb , vw =[],[]\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      vb.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      vw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "    mw=list(vw)\n",
        "    mb=list(vb)\n",
        "\n",
        "    \n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)\n",
        "      beta1_t,beta2_t = beta1,beta2\n",
        "      \n",
        "      for k in range(0,N):\n",
        "         \n",
        "\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          for i in range(len(self.W)):\n",
        "            mw[i]=  beta1_t*mw[i] + (1-beta1_t)*dW[i]\n",
        "            mb[i]=  beta1_t*mb[i] + (1-beta1_t)*db[i]\n",
        "            vb[i]= beta2_t*vb[i] + (1-beta2_t)*db[i]**2\n",
        "            vw[i]= beta2_t*vw[i] + (1-beta2_t)*dW[i]**2\n",
        "\n",
        "            mw_hat=mw[i]/(1-np.power(beta1_t,e+1))\n",
        "            mb_hat=mb[i]/(1-np.power(beta1_t,e+1))\n",
        "            vw_hat=vw[i]/(1-np.power(beta2_t,e+1))\n",
        "            vb_hat=vb[i]/(1-np.power(beta2_t,e+1))\n",
        "          \n",
        "            result_b = self.b[i] - step_size*mb_hat/(np.sqrt(vb_hat)+epsilon)\n",
        "            result_w = self.W[i] - step_size*mw_hat/(np.sqrt(vw_hat)+epsilon)\n",
        "            self.b[i]=result_b\n",
        "            self.W[i]=result_w\n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "  \n",
        "  def nadam(self,step_size,batch_size,epoch,beta1=0.9,beta2=0.999,reg=0.005,epsilon=1e-4):\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    rate = step_size\n",
        "    vb , vw =[],[]\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      vb.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      vw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "    mw=list(vw)\n",
        "    mb=list(vb)\n",
        "\n",
        "    \n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)\n",
        "      beta1_t,beta2_t = beta1,beta2\n",
        "      start_time = time.time()\n",
        "      for k in range(0,N):\n",
        "         \n",
        "\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "         \n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "         \n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "          \n",
        "\n",
        "          for i in range(len(self.W)):\n",
        "            mw[i]=  beta1_t*mw[i] + (1-beta1_t)*dW[i]\n",
        "            mb[i]=  beta1_t*mb[i] + (1-beta1_t)*db[i]\n",
        "            vb[i]= beta2_t*vb[i] + (1-beta2_t)*db[i]**2\n",
        "            vw[i]= beta2_t*vw[i] + (1-beta2_t)*dW[i]**2\n",
        "\n",
        "            mw_hat=mw[i]/(1-np.power(beta1_t,e+1))\n",
        "            mb_hat=mb[i]/(1-np.power(beta1_t,e+1))\n",
        "            vw_hat=vw[i]/(1-np.power(beta2_t,e+1))\n",
        "            vb_hat=vb[i]/(1-np.power(beta2_t,e+1))\n",
        "          \n",
        "            result_w = self.W[i] -(step_size/np.sqrt(vw_hat+epsilon))*(beta1_t*mw_hat+(1-beta1_t)*dW[i]/(1-beta1_t**(e+1)))\n",
        "            result_b = self.b[i] -(step_size/np.sqrt(vb_hat+epsilon))*(beta1_t*mb_hat+(1-beta1_t)*db[i]/(1-beta1_t**(e+1)))\n",
        "            self.b[i]=result_b\n",
        "            self.W[i]=result_w\n",
        "      \n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "\n",
        "\n",
        "  def activation_fun(self,matrix):\n",
        "      if(self.activation==\"sigmoid\"):\n",
        "        return self.sigmoid(matrix)\n",
        "      if(self.activation==\"tanh\"):\n",
        "        return self.tanh(matrix)\n",
        "      if(self.activation==\"ReLU\"):\n",
        "        return self.relu(matrix)\n",
        "  \n",
        "  def activation_derivative(self,matrix):\n",
        "      if(self.activation==\"sigmoid\"):\n",
        "        return self.sigmoid_derivative(matrix)\n",
        "      if(self.activation==\"tanh\"):\n",
        "        return self.tanh_derivative(matrix)\n",
        "      if(self.activation==\"ReLU\"):\n",
        "        return self.relu_derivative(matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question.4**"
      ],
      "metadata": {
        "id": "57JWtdByTpyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \n",
        "\n",
        "    \"method\": 'bayes',\n",
        "    \"metric\":{\n",
        "        'name':'accuracy',\n",
        "        'goal':'maximize'\n",
        "    },\n",
        "    'parameters' :{\n",
        "        \"weight_init\" :{\"values\":[\"random\",\"Xavier\"]},\n",
        "        \"hidden_layers\": {\"values\": [ 3,4,5,6]},\n",
        "        \"size_of_layer\": {\"values\": [ 32, 64,128]},\n",
        "        \"activation\": {\"values\": [\"sigmoid\", \"ReLU\",\"tanh\"]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\",\"momentum\",\"nesterov\", \"adam\",\"rmsprop\",\"nadam\"]},\n",
        "        \"learning_rate\": {\"values\": [0.001,0.0001,0.00001]},\n",
        "        \"epoch\": {\"values\": [5,10]},\n",
        "        \"batch_size\": {\"values\": [16,32,64]},\n",
        "        \"weight_decay\": {\"values\": [0.0005, 0.005, 0.05]}\n",
        "    }\n",
        "    \n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def train_nn():\n",
        "\n",
        "    config_default={\n",
        "    'weight_init':\"random\",\n",
        "    'hidden_layers':3,\n",
        "    'size_of_layer':32,\n",
        "    'activation':\"sigmoid\",\n",
        "    'optimizer':\"sgd\",\n",
        "    'learning_rate':0.01,\n",
        "    'epoch':10,\n",
        "    'batch_size':32,\n",
        "    'weight_decay':0\n",
        "    }\n",
        "    \n",
        "   \n",
        "    wandb.init(config=config_default)\n",
        "    config = wandb.config\n",
        "    name='init_'+str(config.weight_init)+'_hl_'+str(config.hidden_layers)+\"_SL_\"+str(config.size_of_layer)+'_BS_'+str(config.batch_size)+\"_LR_\"+str(config.learning_rate)+'_AF_'+str(config.activation)+'_OPT_'+str(config.optimizer)+'_epoch_'+str(config.epoch)\n",
        "    wandb.init(name=name)\n",
        "    Net = neural_network(train_data,train_labels,test_data,test_labels)\n",
        "    Net.train(epoch=config.epoch, hidden_layers=config.hidden_layers, size_of_layer=config.size_of_layer, batch_size=config.batch_size, activation=config.activation, optimizer=config.optimizer, weight_init=config.weight_init, learning_rate=config.learning_rate, weight_decay=config.weight_decay,loss=\"MSE\")\n",
        "    \n",
        "    \n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"Assignment1(MSE+Bayes)\")\n",
        "wandb.agent(sweep_id, function = train_nn)\n"
      ],
      "metadata": {
        "id": "HD8pkJeDAW5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config_default={\n",
        "    'weight_init':\"Xavier\",\n",
        "    'hidden_layers':5,\n",
        "    'size_of_layer':128,\n",
        "    'activation':\"tanh\",\n",
        "    'optimizer':\"nadam\",\n",
        "    'learning_rate':0.001,\n",
        "    'epoch':10,\n",
        "    'batch_size':16,\n",
        "    'weight_decay':0.005\n",
        "    }\n",
        "\n",
        "   \n",
        "wandb.init(config=config_default)\n",
        "config = wandb.config\n",
        "name='init_'+str(config.weight_init)+'_hl_'+str(config.hidden_layers)+\"_SL_\"+str(config.size_of_layer)+'_BS_'+str(config.batch_size)+\"_LR_\"+str(config.learning_rate)+'_AF_'+str(config.activation)+'_OPT_'+str(config.optimizer)+'_epoch_'+str(config.epoch)\n",
        "Net = neural_network(train_data,train_labels,test_data,test_labels)\n",
        "\n",
        "wandb.init(project = \"Best_sweep\" ,name =name) \n",
        "Net.train(epoch=config.epoch, hidden_layers=config.hidden_layers, size_of_layer=config.size_of_layer, batch_size=config.batch_size, activation=config.activation, optimizer=config.optimizer, weight_init=config.weight_init, learning_rate=config.learning_rate, weight_decay=config.weight_decay)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y_pred=Net.predict(Net.test_data)\n",
        "y_true = Net.test_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "pQmgOmrtdXSl",
        "outputId": "54cce145-e69c-46ab-f59b-18d7ca229924"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230315_131151-1jlxgqhs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vilgax/uncategorized/runs/1jlxgqhs' target=\"_blank\">northern-totem-5</a></strong> to <a href='https://wandb.ai/vilgax/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vilgax/uncategorized' target=\"_blank\">https://wandb.ai/vilgax/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vilgax/uncategorized/runs/1jlxgqhs' target=\"_blank\">https://wandb.ai/vilgax/uncategorized/runs/1jlxgqhs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:1jlxgqhs) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">northern-totem-5</strong> at: <a href='https://wandb.ai/vilgax/uncategorized/runs/1jlxgqhs' target=\"_blank\">https://wandb.ai/vilgax/uncategorized/runs/1jlxgqhs</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230315_131151-1jlxgqhs/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:1jlxgqhs). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230315_131153-676545ms</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vilgax/Best_sweep/runs/676545ms' target=\"_blank\">init_Xavier_hl_5_SL_128_BS_16_LR_0.001_AF_tanh_OPT_nadam_epoch_10</a></strong> to <a href='https://wandb.ai/vilgax/Best_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vilgax/Best_sweep' target=\"_blank\">https://wandb.ai/vilgax/Best_sweep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vilgax/Best_sweep/runs/676545ms' target=\"_blank\">https://wandb.ai/vilgax/Best_sweep/runs/676545ms</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "\n",
        "\n",
        "# Define the confusion matrix and class labels\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "classes = [\"T-Shirt/Top\",\"Trouser\",\"Pullover\",\"Dress\",\"Shirts\",\"Sandal\",\"Coat\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the percentages\n",
        "percentages = (cm / np.sum(cm)) * 100\n",
        "\n",
        "# Define the text for each cell\n",
        "cell_text = []\n",
        "for i in range(len(classes)):\n",
        "    row_text = []\n",
        "    for j in range(len(classes)):\n",
        "\n",
        "        txt = \"Total \"+f'{cm[i, j]}<br>Per. ({percentages[i, j]:.3f})'\n",
        "        if(i==j):\n",
        "          txt =\"Correcty Predicted \" +classes[i]+\"<br>\"+txt\n",
        "        if(i!=j):\n",
        "          txt =\"Predicted \" +classes[j]+\" For \"+classes[i]+\"<br>\"+txt\n",
        "        row_text.append(txt)\n",
        "    cell_text.append(row_text)\n",
        "\n",
        "# Define the trace\n",
        "trace = go.Heatmap(z=percentages,\n",
        "                   x=classes,\n",
        "                   y=classes,\n",
        "                   colorscale='Blues',\n",
        "                   colorbar=dict(title='Percentage'),\n",
        "                   hovertemplate='%{text}%<extra></extra>',\n",
        "                   text=cell_text,\n",
        "                   )\n",
        "\n",
        "# Define the layout\n",
        "layout = go.Layout(title='Confusion Matrix',\n",
        "                   xaxis=dict(title='Predicted Classes'),\n",
        "                   yaxis=dict(title='True Classes'),\n",
        "                   )\n",
        "\n",
        "# Plot the figure\n",
        "fig = go.Figure(data=[trace], layout=layout)\n",
        "wandb.log({'confusion_matrix': (fig)})"
      ],
      "metadata": {
        "id": "B2-RCyRJJDnP"
      },
      "execution_count": 4,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7/erUrUqIporM14otoeQy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}