{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdpW5SUS5IGnDxCiFJi1X/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam10816/CS22M082_ASSIGNEMNT_1/blob/q2/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hhgoo0AzGUZy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "train_data = np.reshape(train_data/255,(len(train_data),train_data.shape[1]**2))\n",
        "test_data = np.reshape(test_data/255,(len(test_data),test_data.shape[1]**2))\n",
        "test_labels =np.reshape(test_labels,(1,len(test_data)))\n"
      ],
      "metadata": {
        "id": "2p4rX01kwJw7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "train_data = np.reshape(train_data/255,(len(train_data),train_data.shape[1]**2))\n",
        "test_data = np.reshape(test_data/255,(len(test_data),test_data.shape[1]**2))\n",
        "test_labels =np.reshape(test_labels,(1,len(test_data)))\n",
        "class neural_network:\n",
        "\n",
        "  #it initializes W and b\n",
        "  def __init__(self,train_data,train_label,hi):\n",
        "    \n",
        "    self.train_data=train_data\n",
        "    self.train_label=train_label\n",
        "    l= train_data.shape[1]\n",
        "\n",
        "    self.W =[self.xavier_init(hi[0],l)] \n",
        "    self.b =[self.xavier_init(1,hi[0])]\n",
        "    for i in range(1,len(hi)) :\n",
        "      self.W.append(self.xavier_init(hi[i],hi[i-1]))\n",
        "      self.b.append(self.xavier_init(1,hi[i])) \n",
        "    self.W.append(self.xavier_init(10,hi[-1]))\n",
        "    \n",
        "    self.b.append(self.xavier_init(1,10))\n",
        "  \n",
        "  def WX_plus_B(self,W, X, b):\n",
        "    \n",
        "    result = np.dot(X, W.transpose())\n",
        "    row_count = result.shape[0]\n",
        "    \n",
        "    row_matrix_repeated = np.tile(b, (row_count, 1))\n",
        "    return result + row_matrix_repeated\n",
        "\n",
        "    \n",
        "  def xavier_init(self,input_size, output_size):\n",
        "    \"\"\"\n",
        "    Initialize weights using Xavier initialization.\n",
        "\n",
        "    Parameters:\n",
        "    input_size (int): number of input units.\n",
        "    output_size (int): number of output units.\n",
        "\n",
        "    Returns:\n",
        "    weights (ndarray): array of shape (input_size, output_size) containing the initialized weights.\n",
        "    \"\"\"\n",
        "    # Calculate the variance of the weights\n",
        "    variance = 2.0 / (input_size + output_size)\n",
        "\n",
        "    # Calculate the standard deviation of the weights\n",
        "    standard_deviation = np.sqrt(variance)\n",
        "\n",
        "    # Generate random weights from a normal distribution with mean 0 and standard deviation standard_deviation\n",
        "    weights = np.random.normal(loc=0, scale=standard_deviation, size=(input_size, output_size))\n",
        "\n",
        "    return weights\n",
        "\n",
        "  #calculates sigmoid for matrix\n",
        "  def sigmoid(self,x):\n",
        "  \n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "  \n",
        "  def sum_columns(self,matrix):\n",
        "    if isinstance(matrix, np.ndarray):\n",
        "        # if matrix is a numpy array, convert it to a list\n",
        "        matrix = matrix.tolist()\n",
        "    \n",
        "    # sum the elements of each column and store in a list\n",
        "    column_sums = [sum(col) for col in zip(*matrix)]\n",
        "    \n",
        "    # convert the list to a 2D matrix of shape (1 x n)\n",
        "    row_matrix = np.array([column_sums])\n",
        "    \n",
        "    return row_matrix\n",
        "  #softmax for matrix\n",
        "  def softmax(self,x):\n",
        "    # Subtract the maximum value in each row from all the values in that row\n",
        "    # to prevent numerical instability from very large or very small values\n",
        "    # in the exponentials of the softmax function.\n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "  \n",
        "  def subtract_matrices(self,W, W_theta, step_size):\n",
        "    \"\"\"\n",
        "    Subtract the matrices in the second list from the matrices in the first list, after multiplying the matrices in the\n",
        "    second list by a step size.\n",
        "\n",
        "    Args:\n",
        "        first_list (list): A list of numpy arrays representing the first set of matrices.\n",
        "        second_list (list): A list of numpy arrays representing the second set of matrices.\n",
        "        step_size (float): The step size to multiply the second set of matrices by.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of numpy arrays representing the result of subtracting the second set of matrices from the first set\n",
        "        of matrices after multiplying the second set of matrices by the step size.\n",
        "    \"\"\"\n",
        "    result_list = []\n",
        "    for i in range(len(self.W)):\n",
        "        result = W[i] - step_size * W_theta[i]\n",
        "        result_list.append(result)\n",
        "    return result_list\n",
        "\n",
        "  def sigmoid_derivative(self,matrix):\n",
        "    \"\"\"\n",
        "    Calculate the derivative of the sigmoid function on a 2D matrix.\n",
        "\n",
        "    Args:\n",
        "        matrix (numpy.ndarray): A numpy array representing the matrix.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A numpy array representing the result of calculating the sigmoid derivative on the matrix.\n",
        "    \"\"\"\n",
        "    shift = np.max(matrix, axis=1, keepdims=True)\n",
        "    exp_matrix = np.exp(matrix - shift)\n",
        "    sig = 1 / (1 + exp_matrix)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "  def forward_pro(self,X):\n",
        "    A=[]\n",
        "    H=[]\n",
        "    A.append(self.WX_plus_B(self.W[0],X,self.b[0])) # a0 = WoX +bo\n",
        "\n",
        "    for i in range(1,len(hi)):\n",
        "\n",
        "      H.append(self.sigmoid(A[i-1])) # hi = g(ai)\n",
        "      #print(H[i-1])\n",
        "      A.append( self.WX_plus_B(self.W[i],H[i-1],self.b[i])) # ai = WiX +bi\n",
        "\n",
        "    H.append(self.sigmoid(A[-2]))\n",
        "    A.append(self.WX_plus_B(self.W[-1],H[-1],self.b[-1]))\n",
        "    \n",
        "    y_hat = self.softmax((A[-1]))\n",
        "    \n",
        "    \n",
        "    \n",
        "    return A,H,y_hat\n",
        "\n",
        "  def back_prop(self,X,Y,A,H,y_hat):\n",
        "    W_theta , b_theta,H_theta,A_theta =self.W,self.b,H,A\n",
        "    #print(np.argmax(y_hat),lable)\n",
        "    ey = np.zeros((y_hat.shape[0],y_hat.shape[1]))\n",
        "\n",
        "    for i in range(0,len(Y)):\n",
        "      ey[i][Y[i]]=1\n",
        "  \n",
        "    L =len(A_theta)\n",
        "    A_theta[L-1]=(-(ey-y_hat))\n",
        "    \n",
        "    #-------------------------\n",
        "    for k in range(L-1,0,-1):\n",
        "      \n",
        "      W_theta[k]=((np.matmul(A_theta[k].transpose(),H[k-1])))/self.batch_size # athetak*h[k-1]\n",
        "      b_theta[k]=self.sum_columns(A_theta[k])/self.batch_size\n",
        "      H_theta[k-1]=np.matmul(A_theta[k],self.W[k])\n",
        "  \n",
        "      A_theta[k-1]=H_theta[k-1]*self.sigmoid_derivative(A[k-1])\n",
        "\n",
        "    W_theta[0] = ((np.matmul(A_theta[0].transpose(),X)))\n",
        "    b_theta[0]=self.sum_columns(A_theta[0])\n",
        "\n",
        "    # print(\"-------W-----\")\n",
        "    # for i in W_theta :\n",
        "    #   print(i)\n",
        "    # print(\"-------b-----\")\n",
        "    # for i in b_theta :\n",
        "    #   print(i)\n",
        "    # print(\"-------A-----\")\n",
        "    # for i in A_theta :\n",
        "    #   print(i)\n",
        "    # print(\"-------H-----\")\n",
        "    # for i in H_theta :\n",
        "    #   print(i)\n",
        "    \n",
        "    return W_theta , b_theta\n",
        "\n",
        "  def accuracy(self, X_test, y_test):\n",
        "    \n",
        "    # Feed forward through the network\n",
        "    A,H,y_hat =self.forward_pro(X_test)\n",
        "    \n",
        "    \n",
        "    y_pred = np.argmax(y_hat, axis=1)\n",
        "   \n",
        "    # Calculate accuracy\n",
        "    acc = np.mean(y_pred == y_test)\n",
        "    # Calculate accuracy\n",
        "    \n",
        "\n",
        "    return acc\n",
        "\n",
        "  def sgd(self,step_size,batch_size,epoch):\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    for e in range(0,epoch):\n",
        "      start_time = time.time()\n",
        "      for k in range(0,N):\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          \n",
        "          A,H,y_hat=self.forward_pro(minibatch)\n",
        "\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "          \n",
        "          self.b =self.subtract_matrices(self.b,db,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,dW,step_size)\n",
        "\n",
        "          \n",
        "          \n",
        "      print(\"epoch \",e ,\"  %s seconds \" % (time.time() - start_time))\n",
        "     \n",
        "      \n",
        "hi=[108,108]   \n",
        "Net = neural_network(train_data,train_labels,hi)\n",
        "print(\"accuracy :-\",Net.accuracy(test_data,test_labels)*100,\"%\")\n",
        "Net.sgd(0.000001,16,50)\n",
        "print(\"accuracy :-\",Net.accuracy(test_data,test_labels)*100,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOS7o3xrG1Yn",
        "outputId": "5a03fdc6-1572-4341-a335-bcee59405dca"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :- 10.0 %\n",
            "epoch  0   7.121453046798706 seconds \n",
            "epoch  1   5.933155059814453 seconds \n",
            "epoch  2   7.5015480518341064 seconds \n",
            "epoch  3   6.035588026046753 seconds \n",
            "epoch  4   7.403289318084717 seconds \n",
            "epoch  5   5.857739210128784 seconds \n",
            "epoch  6   7.534205436706543 seconds \n",
            "epoch  7   6.048337936401367 seconds \n",
            "epoch  8   7.6824116706848145 seconds \n",
            "epoch  9   5.81597638130188 seconds \n",
            "epoch  10   7.425573825836182 seconds \n",
            "epoch  11   5.81052041053772 seconds \n",
            "epoch  12   7.5280327796936035 seconds \n",
            "epoch  13   5.874327182769775 seconds \n",
            "epoch  14   7.220957040786743 seconds \n",
            "epoch  15   6.032543182373047 seconds \n",
            "epoch  16   7.249237537384033 seconds \n",
            "epoch  17   6.6911773681640625 seconds \n",
            "epoch  18   6.4222259521484375 seconds \n",
            "epoch  19   6.7753074169158936 seconds \n",
            "epoch  20   6.438999176025391 seconds \n",
            "epoch  21   7.526404857635498 seconds \n",
            "epoch  22   5.938875913619995 seconds \n",
            "epoch  23   7.358071327209473 seconds \n",
            "epoch  24   5.860041856765747 seconds \n",
            "epoch  25   7.7184388637542725 seconds \n",
            "epoch  26   5.998617649078369 seconds \n",
            "epoch  27   7.458719730377197 seconds \n",
            "epoch  28   5.824296474456787 seconds \n",
            "epoch  29   7.560678482055664 seconds \n",
            "epoch  30   6.024571657180786 seconds \n",
            "epoch  31   7.358548879623413 seconds \n",
            "epoch  32   5.918223857879639 seconds \n",
            "epoch  33   7.405067205429077 seconds \n",
            "epoch  34   6.213688135147095 seconds \n",
            "epoch  35   7.616018056869507 seconds \n",
            "epoch  36   8.004152059555054 seconds \n",
            "epoch  37   7.366327524185181 seconds \n",
            "epoch  38   7.209759712219238 seconds \n",
            "epoch  39   5.938396692276001 seconds \n",
            "epoch  40   7.469865322113037 seconds \n",
            "epoch  41   6.130714416503906 seconds \n",
            "epoch  42   7.586554288864136 seconds \n",
            "epoch  43   5.937602519989014 seconds \n",
            "epoch  44   7.341298580169678 seconds \n",
            "epoch  45   6.125306844711304 seconds \n",
            "epoch  46   7.429747819900513 seconds \n",
            "epoch  47   6.370173692703247 seconds \n",
            "epoch  48   7.680598258972168 seconds \n",
            "epoch  49   6.603752851486206 seconds \n",
            "accuracy :- 10.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(Net.W[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81ZgkBC9_bGE",
        "outputId": "acc9259b-6a7c-4739-eae6-675c92ceffd6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00000000e+00, 0.00000000e+00, 4.31060806e-07, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 4.31060806e-07, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 4.31060806e-07, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       ...,\n",
              "       [0.00000000e+00, 0.00000000e+00, 4.31060806e-07, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 4.31060806e-07, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 4.31060806e-07, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "b =[np.ones(1,50)]\n",
        "\n",
        "# Net.sgd(0.0001,16,100)\n",
        "#Net.calculate_accuracy(test_data,test_labels)"
      ],
      "metadata": {
        "id": "pUXXVOUXt91X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "train_data = np.reshape(train_data/255,(len(train_data),train_data.shape[1]**2))\n",
        "test_data = np.reshape(test_data/255,(len(test_data),test_data.shape[1]**2))\n",
        "test_labels =np.reshape(test_labels,(1,len(test_data)))\n"
      ],
      "metadata": {
        "id": "w5ldCCJ-Jxy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data.shape)\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "id": "aysDESkXrvaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T2nJHtRzKJeg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}