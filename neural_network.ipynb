{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam10816/cs6910_assignment1/blob/main/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hhgoo0AzGUZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9ee365f-f5fd-4527-e4ec-3245c947b49c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.14.0)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init(project = \"Assignment 1_new\" ,name = \"Question 1\")\n",
        "# fig,axs = plt.subplots(2,5,figsize=(20,6))\n",
        "# axs =axs.flatten()\n",
        "# images=[]\n",
        "# for i in range(0,10):\n",
        "#   index =random.choice(np.where(train_labels==i)[0])\n",
        "  \n",
        "#   axs[i].imshow(train_data[index],cmap=\"gray\")\n",
        "#   axs[i].set_title(titles[i])\n",
        "#   Img = wandb.Image(train_data[index],caption=[titles[i]])\n",
        "#   images.append(Img)\n",
        "# wandb.log({\"examples\":images})\n"
      ],
      "metadata": {
        "id": "CG7IszZv9655"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2/3**"
      ],
      "metadata": {
        "id": "dmMY0qTlUMea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qOS7o3xrG1Yn"
      },
      "outputs": [],
      "source": [
        "from numpy.core.multiarray import ndarray\n",
        "\n",
        "import wandb\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "#---------------------------import fashion mnist data---------------------------\n",
        "\n",
        "class neural_network:\n",
        "\n",
        "  \n",
        "  #-------------------------Constructor to take train_data and test_data--------\n",
        "  def __init__(self,train_data,train_labels,test_data,test_labels):\n",
        "    \n",
        "    #-----------------------Randomize and Normalize the data----------------------------------\n",
        "\n",
        "    train_data = np.reshape(train_data/255.0,(len(train_data),train_data.shape[1]**2))\n",
        "\n",
        "    # combine train_data and train_label into a single array\n",
        "\n",
        "    train_data_label = np.column_stack((train_data, train_labels))\n",
        "\n",
        "    # shuffle the rows of the combined array in unison\n",
        "    np.random.shuffle(train_data_label)\n",
        "\n",
        "    # separate the shuffled array back into train_data and train_label\n",
        "    train_data = train_data_label[:, :-1]\n",
        "    train_labels = train_data_label[:, -1]\n",
        "    train_labels = train_labels.astype(np.int32) # Convert labels to int32 type\n",
        "\n",
        "    self.test_data = np.reshape(test_data/255.0,(len(test_data),test_data.shape[1]**2))\n",
        "    self.test_labels =test_labels \n",
        "\n",
        "    #-----------------------Split data in train/validation set(90:10)-----------\n",
        "\n",
        "    l=int(train_data.shape[0]/100)*90\n",
        "    self.train_data=train_data[0:l]\n",
        "    self.train_label=train_labels[0:l]\n",
        "    self.validation_data = train_data[l:]\n",
        "    self.validation_label = train_labels[l:]\n",
        "   \n",
        "   \n",
        "    \n",
        "  #------Train function to fit neural network with different parameter----------\n",
        "\n",
        "  def train(self,weight_init=\"random\",hidden_layers=1,size_of_layer=4,activation=\"sigmoid\",optimizer=\"sgd\",learning_rate=0.1,epoch=1,batch_size=4,weight_decay=0.0,loss=\"cross_entropy\",momentum=0.9,beta =0.9,beta1=0.9,beta2=0.999,epsilon=0.000001):\n",
        "    \n",
        "\n",
        "    #-----------------------Weight Initialization-------------------------------\n",
        "    np.random.seed(42)\n",
        "    self.loss = loss\n",
        "    self.hi=[size_of_layer]*hidden_layers\n",
        "    self.activation =activation\n",
        "    if(weight_init==\"Xavier\"):\n",
        "      self.xav()\n",
        "    elif(weight_init==\"random\"):\n",
        "      self.rndm()\n",
        "\n",
        "    #-------------------------------------Optimizer-----------------------------\n",
        "    if(optimizer==\"sgd\"):\n",
        "      self.sgd(step_size=learning_rate,batch_size =batch_size,epoch=epoch,reg=weight_decay)\n",
        "    elif(optimizer==\"momentum\"):\n",
        "      self.mbgd(step_size=learning_rate,batch_size =batch_size,epoch=epoch,beta=momentum,reg=weight_decay)\n",
        "    elif(optimizer==\"nesterov\"):\n",
        "      self.nagd(step_size=learning_rate,batch_size =batch_size,epoch=epoch,beta=momentum,reg=weight_decay)\n",
        "    elif(optimizer==\"rmsprop\"):\n",
        "      self.rmsprop(step_size=learning_rate,batch_size =batch_size,epoch=epoch,beta=beta,reg=weight_decay)\n",
        "    elif(optimizer==\"adam\"):\n",
        "      self.adam(step_size=learning_rate,batch_size =batch_size,epoch=epoch,beta1=beta1,beta2=beta2,reg=weight_decay)\n",
        "    elif(optimizer==\"nadam\"):\n",
        "      self.nadam(step_size=learning_rate,batch_size =batch_size,epoch=epoch,beta1=beta1,beta2=beta2,reg=weight_decay)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "  #------------------------------Xavier Initialization(W & b)-------------------\n",
        "\n",
        "  def xav(self):\n",
        "    l= self.train_data.shape[1]\n",
        "\n",
        "    self.W =[self.xavier_init(self.hi[0],l)] \n",
        "    self.b =[self.xavier_init(1,self.hi[0])]\n",
        "    for i in range(1,len(self.hi)) :\n",
        "      self.W.append(self.xavier_init(self.hi[i],self.hi[i-1]))\n",
        "      self.b.append(self.xavier_init(1,self.hi[i])) \n",
        "    self.W.append(self.xavier_init(10,self.hi[-1]))\n",
        "    \n",
        "    self.b.append(self.xavier_init(1,10))\n",
        "  \n",
        "\n",
        "  #-----------------xavier_init_function(Return Matrix of (n,m))----------------\n",
        "\n",
        "  def xavier_init(self,n, m):\n",
        "    # Calculate the Xavier initialization scale factor\n",
        "    xavier_scale = np.sqrt(2.0 / (n + m))\n",
        "\n",
        "    # Use numpy's random function to generate a matrix of shape (n, m)\n",
        "    matrix = np.random.randn(n, m) * xavier_scale\n",
        "\n",
        "    return matrix\n",
        "\n",
        "  #-----------------------------Random Initialization(W & b)--------------------\n",
        "\n",
        "  def rndm(self):\n",
        "    l= self.train_data.shape[1]\n",
        "\n",
        "    self.W =[np.random.randn(self.hi[0],l)] \n",
        "    self.b =[np.random.randn(1,self.hi[0])]\n",
        "    for i in range(1,len(self.hi)) :\n",
        "      self.W.append(np.random.randn(self.hi[i],self.hi[i-1]))\n",
        "      self.b.append(np.random.randn(1,self.hi[i])) \n",
        "    self.W.append(np.random.randn(10,self.hi[-1]))\n",
        "    \n",
        "    self.b.append(np.random.randn(1,10))\n",
        "\n",
        "  #--------------------------Relu Activation function---------------------------\n",
        "\n",
        "  def relu(self,matrix):\n",
        "    return np.maximum(matrix, 0) \n",
        "\n",
        "  #--------------------------Relu Activation Derivation function---------------\n",
        "\n",
        "  def relu_derivative(self,matrix):\n",
        "    \n",
        "    # Create a copy of the input matrix and convert to float\n",
        "    derivative = np.array(matrix, dtype=np.float64)\n",
        "    \n",
        "    # Set negative values to 0\n",
        "    derivative[derivative < 0] = 0\n",
        "    \n",
        "    # Set positive values to 1\n",
        "    derivative[derivative > 0] = 1\n",
        "\n",
        "    return derivative\n",
        "\n",
        "  #--------------------------Tanh Activation function---------------------------\n",
        "\n",
        "  def tanh(self,matrix):\n",
        "    \n",
        "    # Avoid overflow by scaling inputs to the range [-100, 100]\n",
        "    x = np.clip(matrix, -100, 100)\n",
        "    \n",
        "    # Apply tanh element-wise\n",
        "    return np.tanh(x)\n",
        "\n",
        "  #-------------------------Tanh Activation Derivative function-----------------\n",
        "  def tanh_derivative(self,matrix):\n",
        "   \n",
        "    # Avoid overflow by scaling inputs to the range [-100, 100]\n",
        "    x = np.clip(matrix, -100, 100)\n",
        "    \n",
        "    # Compute tanh element-wise\n",
        "    tanh_x = np.tanh(x)\n",
        "    \n",
        "    # Compute derivative of tanh element-wise\n",
        "    derivative = 1 - tanh_x**2\n",
        "    \n",
        "    \n",
        "    \n",
        "    return derivative\n",
        "\n",
        "  #-------------------------calculates sigmoid for matrix-----------------------\n",
        "\n",
        "  def sigmoid(self,x):\n",
        "  \n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "  #------------------------sigmoid_derivative_function--------------------------\n",
        "\n",
        "  def sigmoid_derivative(self,matrix):\n",
        "   \n",
        "    shift = np.max(matrix, axis=1, keepdims=True)\n",
        "    exp_matrix = np.exp(matrix - shift)\n",
        "    sig = 1 / (1 + exp_matrix)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "  #-------------------------WX_plus_B function----------------------------------\n",
        "\n",
        "  def WX_plus_B(self,W, X, b):\n",
        "    \n",
        "    #matrix multiply\n",
        "    result = np.dot(X, W.transpose())\n",
        "\n",
        "    \n",
        "    #make b of shape result\n",
        "    row_count = result.shape[0]\n",
        "    row_matrix_repeated = np.tile(b, (row_count, 1))\n",
        "\n",
        "    return result + row_matrix_repeated\n",
        "\n",
        "  #----------------------------sum each element of column-----------------------\n",
        "\n",
        "  def sum_columns(self,matrix):\n",
        "    if isinstance(matrix, np.ndarray):\n",
        "        # if matrix is a numpy array, convert it to a list\n",
        "        matrix = matrix.tolist()\n",
        "    \n",
        "    # sum the elements of each column and store in a list\n",
        "    column_sums = [sum(col) for col in zip(*matrix)]\n",
        "    \n",
        "    # convert the list to a 2D matrix of shape (1 x n)\n",
        "    row_matrix = np.array([column_sums])\n",
        "    \n",
        "    return row_matrix\n",
        "\n",
        "  #----------------softmax function for matrix----------------------------------\n",
        "\n",
        "  def softmax(self,x):\n",
        "    \n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "  \n",
        "  #-----------Substract Matrix (W -step_size*W_theta)---------------------------\n",
        "\n",
        "  def subtract_matrices(self,W, W_theta, step_size):\n",
        "    \n",
        "    result_list = []\n",
        "    for i in range(len(self.W)):\n",
        "        result = W[i] - step_size * (W_theta[i])\n",
        "        result_list.append(result)\n",
        "    return result_list\n",
        "\n",
        "  #------------------Mean square error function---------------------------------\n",
        "  def mean_squared_error(self,y_hat, y):\n",
        "    n = y_hat.shape[0]  # number of samples\n",
        "    k = y.astype(int)  # convert y to integer type for indexing\n",
        "    y_k = np.zeros((n, 10))  # create a one-hot encoding of y\n",
        "    y_k[np.arange(n), k] = 1\n",
        "    \n",
        "    # Calculate mean squared error\n",
        "    mse = np.mean((y_hat - y_k)**2)\n",
        "    \n",
        "    return mse\n",
        "  \n",
        "  #-------------------Mean square error Derivative function---------------------\n",
        "\n",
        "  def mean_squared_error_derivative(self,y_hat, y):\n",
        "    n = y_hat.shape[0]  # number of samples\n",
        "    k = y.astype(int)  # convert y to integer type for indexing\n",
        "    y_k = np.zeros((n, 10))  # create a one-hot encoding of y\n",
        "    y_k[np.arange(n), k] = 1\n",
        "    \n",
        "    # Calculate derivative\n",
        "    dMSE_dy_hat = (2/n) * (y_hat - y_k)\n",
        "    \n",
        "    return dMSE_dy_hat\n",
        "  \n",
        "  #--------------------cross_entropy_loss derivative----------------------------\n",
        "\n",
        "  def cross_entropy_loss_derivative(self,y_hat, Y):\n",
        "    ey = np.zeros((y_hat.shape[0],y_hat.shape[1]))\n",
        "\n",
        "    for i in range(0,len(Y)):\n",
        "      ey[i][Y[i]]=1\n",
        "    \n",
        "    return (-(ey-y_hat))\n",
        "\n",
        "  #---------------------Predict labels for Input Data---------------------------\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    A,H,y_hat =self.forward_pro(X_test,self.W,self.b)\n",
        "  \n",
        "    y_pred = np.argmax(y_hat, axis=1)\n",
        "    return y_pred\n",
        "  \n",
        "  #----------------------Forward Propogation Code-------------------------------\n",
        "\n",
        "  def forward_pro(self,X,W,b):\n",
        "\n",
        "    \n",
        "    A=[]\n",
        "    H=[]\n",
        "    A.append(self.WX_plus_B(W[0],X,b[0])) # a0 = WoX +bo\n",
        "    \n",
        "    for i in range(1,len(self.hi)):\n",
        "\n",
        "      H.append(self.activation_fun(A[-1])) # hi = g(ai)\n",
        "     \n",
        "      A.append( self.WX_plus_B(W[i],H[-1],b[i])) # ai = WiX +bi\n",
        "\n",
        "    H.append(self.activation_fun(A[-1]))         # hi  = g(ai)\n",
        "    A.append(self.WX_plus_B(W[-1],H[-1],b[-1]))  # ai = WiX +bi\n",
        "    \n",
        "    #--------apply softmax function for final probbilities----------------------\n",
        "    y_hat = self.softmax((A[-1]))\n",
        "\n",
        "    return A,H,y_hat\n",
        "  \n",
        "  #----------------Back Proppogation function-----------------------------------\n",
        "\n",
        "  def back_prop(self,X,Y,A,H,y_hat):\n",
        "\n",
        "\n",
        "    W_theta , b_theta,H_theta,A_theta =[],[],[],[]\n",
        "    \n",
        "    L =len(A)\n",
        "\n",
        "    w_2 =0;\n",
        "\n",
        "    for M in self.W:\n",
        "      w_2+=np.sum(np.square(M))\n",
        "    \n",
        "    w_2/=len(self.train_data);\n",
        "\n",
        "    #------------------------Check if loss is cross_entropy---------------------\n",
        "    if(self.loss==\"cross_entropy\"):\n",
        "      lw =self.cross_entropy_loss_derivative(y_hat,Y)\n",
        "      lw+= (self.reg/2)*w_2\n",
        "      A_theta.append(lw)\n",
        "    if(self.loss==\"MSE\"):\n",
        "\n",
        "      lw =self.mean_squared_error_derivative(y_hat,Y)\n",
        "      lw+= (self.reg/2)*w_2\n",
        "      A_theta.append(lw)\n",
        "      \n",
        "    \n",
        "    #-------------------------W_theta , b_theta calculation---------------------\n",
        "\n",
        "    for k in range(L-1,0,-1):\n",
        "      \n",
        "      W_theta.append((np.matmul(A_theta[-1].transpose(),H[k-1])+self.reg*self.W[k]) ) # athetak*h[k-1]\n",
        "      b_theta.append( self.sum_columns(A_theta[-1]))                                \n",
        "      H_theta.append(np.matmul(A_theta[-1],self.W[k]))\n",
        "  \n",
        "      A_theta.append(H_theta[-1]*self.activation_derivative(A[k-1]))\n",
        "\n",
        "    W_theta.append((np.matmul(A_theta[-1].transpose(),X)+self.reg*self.W[0]))\n",
        "    b_theta.append(self.sum_columns(A_theta[-1]))\n",
        "\n",
        "    W_theta.reverse()\n",
        "    b_theta.reverse()\n",
        "\n",
        "    return W_theta , b_theta\n",
        "\n",
        "  #-------------------------Accuracy Calculation--------------------------------\n",
        "\n",
        "  def accuracy(self, X_test, y_test):\n",
        "    \n",
        "    # Feed forward through the network\n",
        "    A,H,y_hat =self.forward_pro(X_test,self.W,self.b)\n",
        "    \n",
        "    y_pred = np.argmax(y_hat, axis=1)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    acc = np.mean(y_pred == y_test)\n",
        "    # Calculate accuracy\n",
        "  \n",
        "    return acc\n",
        "\n",
        "  #---------------------------Cross Entropy Function----------------------------\n",
        "  def cross_entropy(self,y_hat,Y):\n",
        "\n",
        "      epsilon = 1e-9\n",
        "      y_hat = np.maximum(y_hat, epsilon) # to avoid invalid divide\n",
        "\n",
        "      sum=0.0;\n",
        "      for i in range(0,len(Y)):\n",
        "        sum+=(-np.log2(y_hat[i][Y[i]]))\n",
        "      sum/= float(len(Y))\n",
        "\n",
        "      return sum\n",
        "  \n",
        "  #-------------------------Stochastic Gradient Descent-------------------------\n",
        "\n",
        "  def sgd(self,step_size,batch_size,epoch,reg=0.9):\n",
        "\n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    rate = step_size\n",
        "\n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)        # Learning Rate Updation\n",
        "      \n",
        "      for k in range(0,N):\n",
        "\n",
        "          # -------------------Create  Batch of particular size-----------------\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #---------------Compute A,H,y_hat-------------------------------------\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "\n",
        "          #---------------Compute dW ,dw using back_propogation-----------------\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          #-------------------------Update W and b------------------------------\n",
        "          self.b =self.subtract_matrices(self.b,db,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,dW,step_size)\n",
        "          \n",
        "      # Calculate validation Loss,validation Accuracy , Training_loss , Training Accuracy    \n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.train_data,self.train_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      #-------Update values To Wandb------\n",
        "\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "      wandb.log({\"epoch\":e+1})\n",
        "\n",
        "  #-------------------------Momentum based gradient descent---------------------\n",
        "\n",
        "  def mbgd(self,step_size,batch_size,epoch,beta=0.9,reg=0.005):\n",
        "\n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "\n",
        "    prev_ub , prev_uw =[],[]\n",
        "\n",
        "    #---------------Initilalize All matrix with 0-------------------------------\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      prev_ub.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      prev_uw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "    \n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)\n",
        "      start_time = time.time()\n",
        "      beta_t=beta\n",
        "      for k in range(0,N):\n",
        "          ub,uw = list(prev_ub),list(prev_uw)\n",
        "\n",
        "          #------------------------Create Minibatch data------------------------\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #--------calculate dW ,db using forward and backword propogation------\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          #-----------------------weight update---------------------------------\n",
        "          for i in range(len(self.W)):\n",
        "            ub[i]= beta_t*prev_ub[i] + db[i]\n",
        "            uw[i]= beta_t*prev_uw[i] + dW[i]\n",
        "          self.b =self.subtract_matrices(self.b,ub,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,uw,step_size)\n",
        "\n",
        "          prev_ub , prev_uw = list(ub),list(uw)\n",
        "\n",
        "      # Calculate val_accuracy ,val_loss ,training accuracy ,training loss\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      #---------Calculate perticular loss----------------\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.train_data,self.train_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      #------------------------Update to Wandb----------------------------------\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "      wandb.log({\"epoch\":e+1})\n",
        "\n",
        "  #-------------------Nesterov Accelerated gradient descent---------------------    \n",
        "  def nagd(self,step_size,batch_size,epoch,beta=0.9,reg=0.005):\n",
        "\n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "\n",
        "    #---------------Initilalize All matrix with 0-------------------------------\n",
        "\n",
        "    prev_ub , prev_uw =[],[]\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      prev_ub.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      prev_uw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "\n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "      beta_t=beta\n",
        "      step_size=rate/(e+1)           #learning rate updation\n",
        "      start_time = time.time()\n",
        "      for k in range(0,N):\n",
        "          ub,uw = list(prev_ub),list(prev_uw)\n",
        "          n_w ,n_b =self.subtract_matrices(self.W,prev_uw,beta_t),self.subtract_matrices(self.b,prev_ub,beta_t)\n",
        "\n",
        "          #------------------------Create Minibatch data------------------------\n",
        "\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #--------calculate dW ,db using forward and backword propogation------\n",
        "          A,H,y_hat=self.forward_pro(minibatch,n_w,n_b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          #-----------------------weight update---------------------------------\n",
        "\n",
        "          for i in range(len(self.W)):\n",
        "            ub[i]= beta_t*prev_ub[i] + db[i]\n",
        "            uw[i]= beta_t*prev_uw[i] + dW[i]\n",
        "          self.b =self.subtract_matrices(self.b,ub,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,uw,step_size)\n",
        "\n",
        "          prev_ub , prev_uw = list(ub),list(uw)\n",
        "      \n",
        "      # Calculate val_accuracy ,val_loss ,training accuracy ,training loss\n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.train_data,self.train_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      #------------------------Update to Wandb----------------------------------\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "      wandb.log({\"epoch\":e+1})\n",
        "\n",
        "  def rmsprop(self,step_size,batch_size,epoch,beta=0.9,reg=0.005,epsilon=1e-10):\n",
        "    \n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "\n",
        "    ub , uw =[],[]\n",
        "\n",
        "    #---------------Initilalize All matrix with 0-------------------------------\n",
        "    for i in range(len(self.W)):\n",
        "      ub.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      uw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "\n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "\n",
        "      # Learning Rate Update\n",
        "      step_size=rate/(e+1)      \n",
        "      \n",
        "      beta_t = beta\n",
        "      for k in range(0,N):\n",
        "         \n",
        "          #------------------------Create Minibatch data------------------------\n",
        "          \n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #--------calculate dW ,db using forward and backword propogation------\n",
        "          \n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          #-----------------------weight update---------------------------------\n",
        "          \n",
        "          for i in range(len(self.W)):\n",
        "            ub[i]= beta_t*ub[i] + (1-beta_t)*(db[i]**2)\n",
        "            uw[i]= beta_t*uw[i] + (1-beta_t)*(dW[i]**2)\n",
        "          \n",
        "          for i in range(len(self.W)):\n",
        "            result_b = self.b[i] - step_size*db[i]/(np.sqrt(ub[i])+epsilon)\n",
        "            result_w = self.W[i] - step_size*dW[i]/(np.sqrt(uw[i])+epsilon)\n",
        "            self.b[i]=result_b\n",
        "            self.W[i]=result_w\n",
        "\n",
        "      #Calculate val_accuracy ,val_loss ,training accuracy ,training loss\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      #---------Calculate perticular loss---------------------------------------\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.train_data,self.train_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      \n",
        "      #------------------------Update to Wandb----------------------------------\n",
        "      \n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "      wandb.log({\"epoch\":e+1})\n",
        "      \n",
        "  \n",
        "  def adam(self,step_size,batch_size,epoch,beta1=0.9,beta2=0.999,reg=0.005,epsilon=1e-4):\n",
        "\n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    rate = step_size\n",
        "    vb , vw =[],[]\n",
        "\n",
        "    #---------------Initilalize All matrix with 0-------------------------------\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      vb.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      vw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "    mw=list(vw)\n",
        "    mb=list(vb)\n",
        "\n",
        "    \n",
        "    for e in range(0,epoch):\n",
        "\n",
        "      # update learning rate\n",
        "      step_size=rate/(e+1)            \n",
        "      beta1_t,beta2_t = beta1,beta2\n",
        "      \n",
        "      for k in range(0,N):\n",
        "         \n",
        "          #------------------------Create Minibatch data------------------------\n",
        "          \n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #--------calculate dW ,db using forward and backword propogation------\n",
        "          \n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          #-----------------------weight update---------------------------------\n",
        "          \n",
        "          for i in range(len(self.W)):\n",
        "            mw[i]=  beta1_t*mw[i] + (1-beta1_t)*dW[i]\n",
        "            mb[i]=  beta1_t*mb[i] + (1-beta1_t)*db[i]\n",
        "            vb[i]= beta2_t*vb[i] + (1-beta2_t)*db[i]**2\n",
        "            vw[i]= beta2_t*vw[i] + (1-beta2_t)*dW[i]**2\n",
        "\n",
        "            mw_hat=mw[i]/(1-np.power(beta1_t,e+1))\n",
        "            mb_hat=mb[i]/(1-np.power(beta1_t,e+1))\n",
        "            vw_hat=vw[i]/(1-np.power(beta2_t,e+1))\n",
        "            vb_hat=vb[i]/(1-np.power(beta2_t,e+1))\n",
        "          \n",
        "            result_b = self.b[i] - step_size*mb_hat/(np.sqrt(vb_hat)+epsilon)\n",
        "            result_w = self.W[i] - step_size*mw_hat/(np.sqrt(vw_hat)+epsilon)\n",
        "            self.b[i]=result_b\n",
        "            self.W[i]=result_w\n",
        "\n",
        "      # Calculate val_accuracy ,val_loss ,training accuracy ,training loss\n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "\n",
        "      train_acc = self.accuracy(self.train_data,self.train_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      #------------------------Update values to Wandb----------------------------------\n",
        "      \n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "      wandb.log({\"epoch\":e+1})\n",
        "  \n",
        "  def nadam(self,step_size,batch_size,epoch,beta1=0.9,beta2=0.999,reg=0.005,epsilon=1e-4):\n",
        "\n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    rate = step_size\n",
        "    vb , vw =[],[]\n",
        "\n",
        "    #---------------Initilalize vb ,vw matrix with 0-------------------------------\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      vb.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      vw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "    mw=list(vw)\n",
        "    mb=list(vb)\n",
        "\n",
        "    \n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)            # update learning rate\n",
        "\n",
        "      beta1_t,beta2_t = beta1,beta2\n",
        "      \n",
        "      for k in range(0,N):\n",
        "         \n",
        "          #------------------------Create Minibatch data------------------------\n",
        "          \n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #--------calculate dW ,db using forward and backword propogation------\n",
        "          \n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "          \n",
        "          #-----------------------weight update---------------------------------\n",
        "          \n",
        "          for i in range(len(self.W)):\n",
        "            mw[i]=  beta1_t*mw[i] + (1-beta1_t)*dW[i]\n",
        "            mb[i]=  beta1_t*mb[i] + (1-beta1_t)*db[i]\n",
        "            vb[i]= beta2_t*vb[i] + (1-beta2_t)*db[i]**2\n",
        "            vw[i]= beta2_t*vw[i] + (1-beta2_t)*dW[i]**2\n",
        "\n",
        "            mw_hat=mw[i]/(1-np.power(beta1_t,e+1))\n",
        "            mb_hat=mb[i]/(1-np.power(beta1_t,e+1))\n",
        "            vw_hat=vw[i]/(1-np.power(beta2_t,e+1))\n",
        "            vb_hat=vb[i]/(1-np.power(beta2_t,e+1))\n",
        "          \n",
        "            result_w = self.W[i] -(step_size/np.sqrt(vw_hat+epsilon))*(beta1_t*mw_hat+(1-beta1_t)*dW[i]/(1-beta1_t**(e+1)))\n",
        "            result_b = self.b[i] -(step_size/np.sqrt(vb_hat+epsilon))*(beta1_t*mb_hat+(1-beta1_t)*db[i]/(1-beta1_t**(e+1)))\n",
        "            self.b[i]=result_b\n",
        "            self.W[i]=result_w\n",
        "      \n",
        "      # Calculate val_accuracy ,val_loss ,training accuracy ,training loss\n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.train_data,self.train_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      #------------------------Update to Wandb----------------------------------\n",
        "      \n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "      wandb.log({\"epoch\":e+1})\n",
        "\n",
        "\n",
        "  # compute matrix and return result according to activation parameter\n",
        "  def activation_fun(self,matrix):\n",
        "\n",
        "      if(self.activation==\"sigmoid\"):\n",
        "        return self.sigmoid(matrix)\n",
        "\n",
        "      elif(self.activation==\"tanh\"):\n",
        "        return self.tanh(matrix)\n",
        "\n",
        "      elif(self.activation==\"ReLU\"):\n",
        "        return self.relu(matrix)\n",
        "  \n",
        "  # compute matrix and return result according to activation parameter\n",
        "  def activation_derivative(self,matrix):\n",
        "\n",
        "      if(self.activation==\"sigmoid\"):\n",
        "        return self.sigmoid_derivative(matrix)\n",
        "\n",
        "      elif(self.activation==\"tanh\"):\n",
        "        return self.tanh_derivative(matrix)\n",
        "\n",
        "      elif(self.activation==\"ReLU\"):\n",
        "        return self.relu_derivative(matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HHnKTA0mKMll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question.4**"
      ],
      "metadata": {
        "id": "57JWtdByTpyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This is configuration for Wandb where we have separate function named train_nn\n",
        "# Which will train  neural network for different swwep id with specific configuration\n",
        "# choosen by Wandb.\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    \n",
        "\n",
        "    \"method\": 'bayes',\n",
        "    \"metric\":{\n",
        "        'name':'accuracy',\n",
        "        'goal':'maximize'\n",
        "    },\n",
        "    'parameters' :{\n",
        "        \"weight_init\" :{\"values\":[\"random\",\"Xavier\"]},\n",
        "        \"hidden_layers\": {\"values\": [ 3,4,5]},\n",
        "        \"size_of_layer\": {\"values\": [ 32, 64,128]},\n",
        "        \"activation\": {\"values\": [\"sigmoid\", \"ReLU\",\"tanh\"]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\",\"momentum\",\"nesterov\", \"adam\",\"rmsprop\",\"nadam\"]},\n",
        "        \"learning_rate\": {\"values\": [0.01,0.001,0.0001]},\n",
        "        \"epoch\": {\"values\": [5,10]},\n",
        "        \"batch_size\": {\"values\": [16,32,64]},\n",
        "        \"weight_decay\": {\"values\": [0.0005,0.005,0.05, 0.5]}\n",
        "    }\n",
        "    \n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def train_nn():\n",
        "\n",
        "    config_default={\n",
        "    'weight_init':\"random\",\n",
        "    'hidden_layers':3,\n",
        "    'size_of_layer':32,\n",
        "    'activation':\"sigmoid\",\n",
        "    'optimizer':\"sgd\",\n",
        "    'learning_rate':0.01,\n",
        "    'epoch':10,\n",
        "    'batch_size':32,\n",
        "    'weight_decay':0\n",
        "    }\n",
        "    \n",
        "   \n",
        "    \n",
        "    wandb.init(config=config_default)\n",
        "    config = wandb.config\n",
        "    \n",
        "    # perticular name of sweep\n",
        "    name='init_'+str(config.weight_init)+'_hl_'+str(config.hidden_layers)+\"_SL_\"+str(config.size_of_layer)+'_BS_'+str(config.batch_size)+\"_LR_\"+str(config.learning_rate)+'_AF_'+str(config.activation)+'_OPT_'+str(config.optimizer)+'_epoch_'+str(config.epoch)\n",
        "    wandb.init(name=name,config=config_default)\n",
        "    \n",
        "\n",
        "    # train Neural Network\n",
        "    Net = neural_network(train_data,train_labels,test_data,test_labels)\n",
        "    Net.train(epoch=config.epoch, hidden_layers=config.hidden_layers, size_of_layer=config.size_of_layer, batch_size=config.batch_size, activation=config.activation, optimizer=config.optimizer, weight_init=config.weight_init, learning_rate=config.learning_rate, weight_decay=config.weight_decay,loss=\"MSE\")\n",
        "    \n",
        "    \n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"Assignment1(trainacccorrected)\")\n",
        "wandb.agent(sweep_id, function = train_nn)\n"
      ],
      "metadata": {
        "id": "HD8pkJeDAW5H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a155332b-2736-4009-a4ca-fab50360fc3e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 5bcybbo1\n",
            "Sweep URL: https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5rm9xksv with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_of_layer: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: Xavier\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230319_090950-5rm9xksv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/5rm9xksv' target=\"_blank\">hardy-sweep-1</a></strong> to <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/5rm9xksv' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/5rm9xksv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:5rm9xksv) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">hardy-sweep-1</strong> at: <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/5rm9xksv' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/5rm9xksv</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230319_090950-5rm9xksv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:5rm9xksv). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230319_090952-5rm9xksv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/5rm9xksv' target=\"_blank\">init_Xavier_hl_4_SL_32_BS_16_LR_0.01_AF_tanh_OPT_momentum_epoch_5</a></strong> to <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/5rm9xksv' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/5rm9xksv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>train_accuracy</td><td></td></tr><tr><td>training_loss</td><td></td></tr><tr><td>validation_accuracy</td><td></td></tr><tr><td>validation_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.75387</td></tr><tr><td>training_loss</td><td>1.02314</td></tr><tr><td>validation_accuracy</td><td>0.75567</td></tr><tr><td>validation_loss</td><td>0.03473</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">init_Xavier_hl_4_SL_32_BS_16_LR_0.01_AF_tanh_OPT_momentum_epoch_5</strong> at: <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/5rm9xksv' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/5rm9xksv</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230319_090952-5rm9xksv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: edkj0w71 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_of_layer: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: Xavier\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230319_091052-edkj0w71</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/edkj0w71' target=\"_blank\">rural-sweep-2</a></strong> to <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/edkj0w71' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/edkj0w71</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:edkj0w71) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rural-sweep-2</strong> at: <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/edkj0w71' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/edkj0w71</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230319_091052-edkj0w71/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:edkj0w71). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230319_091054-edkj0w71</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/edkj0w71' target=\"_blank\">init_Xavier_hl_4_SL_32_BS_32_LR_0.01_AF_sigmoid_OPT_rmsprop_epoch_5</a></strong> to <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/sweeps/5bcybbo1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/edkj0w71' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/edkj0w71</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352,
          "referenced_widgets": [
            "cc98e0d7a9064b6084a7edac60990a66",
            "96abae3e81824752928b33689c68cb12",
            "1705f7a2e8ab433c9206e0d6d4903dbf",
            "6ef9e69316ac48889db05035e0bff764",
            "726f0497fcb445b8858ab9e6ccb145aa",
            "6f54d1b9352b4f979186cd51bf0a7208",
            "6f7f8ae3f65d48a59bc799a15783d546",
            "0dfea26c6f734e8da3c05120db1e892d"
          ]
        },
        "id": "eT9Pq4Ch0G3n",
        "outputId": "c2f1cdc4-1c04-4852-fb6a-0c0225e90850"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc98e0d7a9064b6084a7edac60990a66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>train_accuracy</td><td></td></tr><tr><td>training_loss</td><td></td></tr><tr><td>validation_accuracy</td><td></td></tr><tr><td>validation_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>train_accuracy</td><td>0.09996</td></tr><tr><td>training_loss</td><td>3.3222</td></tr><tr><td>validation_accuracy</td><td>0.10033</td></tr><tr><td>validation_loss</td><td>0.09001</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">init_Xavier_hl_4_SL_32_BS_32_LR_0.01_AF_sigmoid_OPT_rmsprop_epoch_5</strong> at: <a href='https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/edkj0w71' target=\"_blank\">https://wandb.ai/vilgax/Assignment1%28trainacccorrected%29/runs/edkj0w71</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230319_091054-edkj0w71/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will train Neural Network with best of configuration and\n",
        "# give ouput y_hat for test_data\n",
        "\n",
        "# Define best configuration from experiments\n",
        "config_default={\n",
        "    'weight_init':\"Xavier\",\n",
        "    'hidden_layers':5,\n",
        "    'size_of_layer':128,\n",
        "    'activation':\"tanh\",\n",
        "    'optimizer':\"nadam\",\n",
        "    'learning_rate':0.001,\n",
        "    'epoch':10,\n",
        "    'batch_size':16,\n",
        "    'weight_decay':0.005\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "wandb.init(config=config_default)\n",
        "config = wandb.config\n",
        "name='init_'+str(config.weight_init)+'_hl_'+str(config.hidden_layers)+\"_SL_\"+str(config.size_of_layer)+'_BS_'+str(config.batch_size)+\"_LR_\"+str(config.learning_rate)+'_AF_'+str(config.activation)+'_OPT_'+str(config.optimizer)+'_epoch_'+str(config.epoch)\n",
        "Net = neural_network(train_data,train_labels,test_data,test_labels)\n",
        "\n",
        "# Fit the model with perticular configuration\n",
        "wandb.init(project = \"Best_sweep\" ,name =name) \n",
        "Net.train(epoch=config.epoch, hidden_layers=config.hidden_layers, size_of_layer=config.size_of_layer, batch_size=config.batch_size, activation=config.activation, optimizer=config.optimizer, weight_init=config.weight_init, learning_rate=config.learning_rate, weight_decay=config.weight_decay)\n",
        "\n",
        "# generate labels for test_data\n",
        "y_pred=Net.predict(Net.test_data)  \n",
        "\n",
        "# get true labels of test data\n",
        "y_true = Net.test_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "pQmgOmrtdXSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "\n",
        "# This code will plot Confusion matrix\n",
        "\n",
        "# Define the confusion matrix and class labels\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "classes = [\"T-Shirt/Top\",\"Trouser\",\"Pullover\",\"Dress\",\"Shirts\",\"Sandal\",\"Coat\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the percentages\n",
        "percentages = (cm / np.sum(cm)) * 100\n",
        "\n",
        "# Define the text for each cell\n",
        "cell_text = []\n",
        "for i in range(len(classes)):\n",
        "    row_text = []\n",
        "    for j in range(len(classes)):\n",
        "\n",
        "        txt = \"Total \"+f'{cm[i, j]}<br>Per. ({percentages[i, j]:.3f})'\n",
        "        if(i==j):\n",
        "          txt =\"Correcty Predicted \" +classes[i]+\"<br>\"+txt\n",
        "        if(i!=j):\n",
        "          txt =\"Predicted \" +classes[j]+\" For \"+classes[i]+\"<br>\"+txt\n",
        "        row_text.append(txt)\n",
        "    cell_text.append(row_text)\n",
        "\n",
        "# Define the trace\n",
        "trace = go.Heatmap(z=percentages,\n",
        "                   x=classes,\n",
        "                   y=classes,\n",
        "                   colorscale='Blues',\n",
        "                   colorbar=dict(title='Percentage'),\n",
        "                   hovertemplate='%{text}%<extra></extra>',\n",
        "                   text=cell_text,\n",
        "                   )\n",
        "\n",
        "# Define the layout\n",
        "layout = go.Layout(title='Confusion Matrix',\n",
        "                   xaxis=dict(title='Predicted Classes'),\n",
        "                   yaxis=dict(title='True Classes'),\n",
        "                   )\n",
        "\n",
        "# Plot the figure\n",
        "fig = go.Figure(data=[trace], layout=layout)\n",
        "wandb.log({'confusion_matrix': (fig)})"
      ],
      "metadata": {
        "id": "B2-RCyRJJDnP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFGq/7j+qGKs4QsZoGV7HQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cc98e0d7a9064b6084a7edac60990a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96abae3e81824752928b33689c68cb12",
              "IPY_MODEL_1705f7a2e8ab433c9206e0d6d4903dbf"
            ],
            "layout": "IPY_MODEL_6ef9e69316ac48889db05035e0bff764"
          }
        },
        "96abae3e81824752928b33689c68cb12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_726f0497fcb445b8858ab9e6ccb145aa",
            "placeholder": "",
            "style": "IPY_MODEL_6f54d1b9352b4f979186cd51bf0a7208",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "1705f7a2e8ab433c9206e0d6d4903dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f7f8ae3f65d48a59bc799a15783d546",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0dfea26c6f734e8da3c05120db1e892d",
            "value": 1
          }
        },
        "6ef9e69316ac48889db05035e0bff764": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "726f0497fcb445b8858ab9e6ccb145aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f54d1b9352b4f979186cd51bf0a7208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f7f8ae3f65d48a59bc799a15783d546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dfea26c6f734e8da3c05120db1e892d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}