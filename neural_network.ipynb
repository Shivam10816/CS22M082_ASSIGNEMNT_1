{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam10816/CS22M082_ASSIGNEMNT_1/blob/q2/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "hhgoo0AzGUZy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "2p4rX01kwJw7"
      },
      "outputs": [],
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "train_data = np.reshape(train_data,(len(train_data),train_data.shape[1]**2))\n",
        "test_data = np.reshape(test_data,(len(test_data),test_data.shape[1]**2))\n",
        "test_labels =np.reshape(test_labels,(1,len(test_data)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qOS7o3xrG1Yn",
        "outputId": "9e6b44ef-1e02-4b1d-8ff3-0642443f72ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "8.32692549149037\n",
            "epoch  0   4.693841218948364 seconds \n",
            "6.4938806849945765\n",
            "epoch  1   3.6675262451171875 seconds \n",
            "4.7037176864039205\n",
            "epoch  2   4.489496946334839 seconds \n",
            "4.011717538756073\n",
            "epoch  3   5.566849231719971 seconds \n",
            "3.6014698359382153\n",
            "epoch  4   3.6727020740509033 seconds \n",
            "3.4865526900406136\n",
            "epoch  5   4.39911150932312 seconds \n",
            "3.4562828033967876\n",
            "epoch  6   4.720358610153198 seconds \n",
            "3.438492816276272\n",
            "epoch  7   3.9348299503326416 seconds \n",
            "3.4199433543971542\n",
            "epoch  8   4.329875946044922 seconds \n",
            "3.401538781448939\n",
            "epoch  9   4.806469440460205 seconds \n",
            "3.3825037336556703\n",
            "epoch  10   3.6316473484039307 seconds \n",
            "3.363241587748884\n",
            "epoch  11   4.154567241668701 seconds \n",
            "3.3437965572857133\n",
            "epoch  12   4.801500558853149 seconds \n",
            "3.3243260171428566\n",
            "epoch  13   3.8326101303100586 seconds \n",
            "3.3049303325975483\n",
            "epoch  14   4.082050561904907 seconds \n",
            "3.285704787737553\n",
            "epoch  15   4.574770212173462 seconds \n",
            "3.266723248267016\n",
            "epoch  16   3.7506611347198486 seconds \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-d18bb5bd8ff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;31m#print(\"accuracy :-\",Net.accuracy(test_data,test_labels)*100,\"%\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m \u001b[0mNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;31m#print(\"accuracy :-\",Net.accuracy(test_data,test_labels)*100,\"%\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-d18bb5bd8ff5>\u001b[0m in \u001b[0;36msgd\u001b[0;34m(self, step_size, batch_size, epoch, reg)\u001b[0m\n\u001b[1;32m    231\u001b[0m           \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m           \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminibatch_lable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m           \u001b[0;31m# print(np.min(dW[0]),np.min(dW[1]),np.min(dW[2]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m           \u001b[0;31m# print(np.min(db[0]),np.min(db[1]),np.min(db[2]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-d18bb5bd8ff5>\u001b[0m in \u001b[0;36mback_prop\u001b[0;34m(self, X, Y, A, H, y_hat)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mW_theta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_theta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mb_theta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_theta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mW_theta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-d18bb5bd8ff5>\u001b[0m in \u001b[0;36msum_columns\u001b[0;34m(self, matrix)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# sum the elements of each column and store in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mcolumn_sums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# convert the list to a 2D matrix of shape (1 x n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "train_data = np.reshape(train_data/255.0,(len(train_data),train_data.shape[1]**2))\n",
        "test_data = np.reshape(test_data/255.0,(len(test_data),test_data.shape[1]**2))\n",
        "test_labels =np.reshape(test_labels,(1,len(test_data)))\n",
        "print(np.max(train_data))\n",
        "class neural_network:\n",
        "\n",
        "  #it initializes W and b\n",
        "  def __init__(self,train_data,train_label,hi):\n",
        "    \n",
        "    self.train_data=train_data\n",
        "    self.train_label=train_label\n",
        "    self.rndm()\n",
        "  \n",
        "  def xav(self):\n",
        "    l= train_data.shape[1]\n",
        "\n",
        "    self.W =[self.xavier_init(hi[0],l)] \n",
        "    self.b =[self.xavier_init(1,hi[0])]\n",
        "    for i in range(1,len(hi)) :\n",
        "      self.W.append(self.xavier_init(hi[i],hi[i-1]))\n",
        "      self.b.append(self.xavier_init(1,hi[i])) \n",
        "    self.W.append(self.xavier_init(10,hi[-1]))\n",
        "    \n",
        "    self.b.append(self.xavier_init(1,10))\n",
        "  \n",
        "\n",
        "  def rndm(self):\n",
        "    l= train_data.shape[1]\n",
        "\n",
        "    self.W =[np.random.randn(hi[0],l)] \n",
        "    self.b =[np.zeros((1,hi[0]))]\n",
        "    for i in range(1,len(hi)) :\n",
        "      self.W.append(np.random.randn(hi[i],hi[i-1]))\n",
        "      self.b.append(np.zeros((1,hi[i]))) \n",
        "    self.W.append(np.random.randn(10,hi[-1]))\n",
        "    \n",
        "    self.b.append(np.random.randn(1,10))\n",
        "  \n",
        "  def WX_plus_B(self,W, X, b):\n",
        "    \n",
        "    result = np.dot(X, W.transpose())\n",
        "    row_count = result.shape[0]\n",
        "    \n",
        "    row_matrix_repeated = np.tile(b, (row_count, 1))\n",
        "    return result + row_matrix_repeated\n",
        "\n",
        "    \n",
        "  def xavier_init(self,input_size, output_size):\n",
        "    \"\"\"\n",
        "    Initialize weights using Xavier initialization.\n",
        "\n",
        "    Parameters:\n",
        "    input_size (int): number of input units.\n",
        "    output_size (int): number of output units.\n",
        "\n",
        "    Returns:\n",
        "    weights (ndarray): array of shape (input_size, output_size) containing the initialized weights.\n",
        "    \"\"\"\n",
        "    # Calculate the variance of the weights\n",
        "    variance = 2.0 / (input_size + output_size)\n",
        "\n",
        "    # Calculate the standard deviation of the weights\n",
        "    standard_deviation = np.sqrt(variance)\n",
        "\n",
        "    # Generate random weights from a normal distribution with mean 0 and standard deviation standard_deviation\n",
        "    weights = np.random.normal(loc=0, scale=standard_deviation, size=(input_size, output_size))\n",
        "\n",
        "    return weights\n",
        "\n",
        "  #calculates sigmoid for matrix\n",
        "  def sigmoid(self,x):\n",
        "  \n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "  \n",
        "  def sum_columns(self,matrix):\n",
        "    if isinstance(matrix, np.ndarray):\n",
        "        # if matrix is a numpy array, convert it to a list\n",
        "        matrix = matrix.tolist()\n",
        "    \n",
        "    # sum the elements of each column and store in a list\n",
        "    column_sums = [sum(col) for col in zip(*matrix)]\n",
        "    \n",
        "    # convert the list to a 2D matrix of shape (1 x n)\n",
        "    row_matrix = np.array([column_sums])\n",
        "    \n",
        "    return row_matrix\n",
        "  #softmax for matrix\n",
        "  def softmax(self,x):\n",
        "    # Subtract the maximum value in each row from all the values in that row\n",
        "    # to prevent numerical instability from very large or very small values\n",
        "    # in the exponentials of the softmax function.\n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "  \n",
        "  def subtract_matrices(self,W, W_theta, step_size):\n",
        "    \"\"\"\n",
        "    Subtract the matrices in the second list from the matrices in the first list, after multiplying the matrices in the\n",
        "    second list by a step size.\n",
        "\n",
        "    Args:\n",
        "        first_list (list): A list of numpy arrays representing the first set of matrices.\n",
        "        second_list (list): A list of numpy arrays representing the second set of matrices.\n",
        "        step_size (float): The step size to multiply the second set of matrices by.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of numpy arrays representing the result of subtracting the second set of matrices from the first set\n",
        "        of matrices after multiplying the second set of matrices by the step size.\n",
        "    \"\"\"\n",
        "    result_list = []\n",
        "    for i in range(len(self.W)):\n",
        "        result = W[i] - step_size * (W_theta[i]/self.batch_size)\n",
        "        result_list.append(result)\n",
        "    return result_list\n",
        "\n",
        "  def sigmoid_derivative(self,matrix):\n",
        "    \"\"\"\n",
        "    Calculate the derivative of the sigmoid function on a 2D matrix.\n",
        "\n",
        "    Args:\n",
        "        matrix (numpy.ndarray): A numpy array representing the matrix.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A numpy array representing the result of calculating the sigmoid derivative on the matrix.\n",
        "    \"\"\"\n",
        "    shift = np.max(matrix, axis=1, keepdims=True)\n",
        "    exp_matrix = np.exp(matrix - shift)\n",
        "    sig = 1 / (1 + exp_matrix)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "  def forward_pro(self,X):\n",
        "    A=[]\n",
        "    H=[]\n",
        "    A.append(self.WX_plus_B(self.W[0],X,self.b[0])) # a0 = WoX +bo\n",
        "\n",
        "    for i in range(1,len(hi)):\n",
        "\n",
        "      H.append(self.sigmoid(A[-1])) # hi = g(ai)\n",
        "      #print(H[i-1])\n",
        "      A.append( self.WX_plus_B(self.W[i],H[-1],self.b[i])) # ai = WiX +bi\n",
        "\n",
        "    H.append(self.sigmoid(A[-1]))\n",
        "    A.append(self.WX_plus_B(self.W[-1],H[-1],self.b[-1]))\n",
        "    \n",
        "    y_hat = self.softmax((A[-1]))\n",
        "    \n",
        "    \n",
        "    \n",
        "    return A,H,y_hat\n",
        "\n",
        "  def back_prop(self,X,Y,A,H,y_hat):\n",
        "    W_theta , b_theta,H_theta,A_theta =[],[],[],[]\n",
        "    #print(np.argmax(y_hat),lable)\n",
        "    ey = np.zeros((y_hat.shape[0],y_hat.shape[1]))\n",
        "\n",
        "    for i in range(0,len(Y)):\n",
        "      ey[i][Y[i]]=1\n",
        "  \n",
        "    L =len(A)\n",
        "    A_theta.append((-(ey-y_hat)))\n",
        "    \n",
        "    #-------------------------\n",
        "    for k in range(L-1,0,-1):\n",
        "      \n",
        "      W_theta.append((np.matmul(A_theta[-1].transpose(),H[k-1])+self.reg*self.W[k]) ) # athetak*h[k-1]\n",
        "      b_theta.append( self.sum_columns(A_theta[-1]))\n",
        "      H_theta.append(np.matmul(A_theta[-1],self.W[k]))\n",
        "  \n",
        "      A_theta.append(H_theta[-1]*self.sigmoid_derivative(A[k-1]))\n",
        "\n",
        "    W_theta.append((np.matmul(A_theta[-1].transpose(),X)+self.reg*self.W[0]))\n",
        "    b_theta.append(self.sum_columns(A_theta[-1]))\n",
        "\n",
        "    W_theta.reverse()\n",
        "    b_theta.reverse()\n",
        "\n",
        "    self.cross_entropy(y_hat,Y)\n",
        "    # print(\"-------W_theta-----\")\n",
        "    # for i in W_theta :\n",
        "    #   print(i.shape)\n",
        "    # print(\"-------b_theta-----\")\n",
        "    # for i in b_theta :\n",
        "    #print(i.shape)\n",
        "    # print(\"-------A-----\")\n",
        "    # for i in A_theta :\n",
        "    #   print(i.shape)\n",
        "    # print(\"-------H-----\")\n",
        "    # for i in H_theta :\n",
        "    #   print(i.shape)\n",
        "    \n",
        "    return W_theta , b_theta\n",
        "\n",
        "  def accuracy(self, X_test, y_test):\n",
        "    \n",
        "    # Feed forward through the network\n",
        "    A,H,y_hat =self.forward_pro(X_test)\n",
        "    \n",
        "    \n",
        "    y_pred = np.argmax(y_hat, axis=1)\n",
        "    print(y_pred.shape)\n",
        "    # Calculate accuracy\n",
        "    acc = np.mean(y_pred == y_test)\n",
        "    # Calculate accuracy\n",
        "    \n",
        "\n",
        "    return acc\n",
        "  def cross_entropy(self,y_hat,Y):\n",
        "      sum=0.0;\n",
        "      for i in range(0,len(Y)):\n",
        "        sum+=(-np.log2(y_hat[i][Y[i]]))\n",
        "      sum/= float(len(Y))\n",
        "      print(sum)\n",
        "  \n",
        "  def sgd(self,step_size,batch_size,epoch,reg):\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    for e in range(0,epoch):\n",
        "      start_time = time.time()\n",
        "      for k in range(0,N):\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          \n",
        "          A,H,y_hat=self.forward_pro(minibatch)\n",
        "\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "          # print(np.min(dW[0]),np.min(dW[1]),np.min(dW[2]))\n",
        "          # print(np.min(db[0]),np.min(db[1]),np.min(db[2]))\n",
        "          # print(np.max(dW[0]),np.max(dW[1]),np.max(dW[2]))\n",
        "          # print(np.max(db[0]),np.max(db[1]),np.max(db[2]))\n",
        "          # print(\"\\n\\n\\n\")\n",
        "          self.b =self.subtract_matrices(self.b,db,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,dW,step_size)\n",
        "          # print(np.min(Net.W[0]),np.min(Net.W[1]),np.min(Net.W[2]))\n",
        "          # print(np.max(Net.W[0]),np.max(Net.W[1]),np.max(Net.W[2]))\n",
        "          # print(np.min(Net.b[0]),np.min(Net.b[1]),np.min(Net.b[2]))\n",
        "\n",
        "          # print(np.max(Net.b[0]),np.max(Net.b[1]),np.max(Net.b[2]))\n",
        "          # print(\"\\n\\n\\n\")\n",
        "          \n",
        "      print(\"epoch \",e ,\"  %s seconds \" % (time.time() - start_time))\n",
        "     \n",
        "      \n",
        "hi=[32,32,32]   \n",
        "Net = neural_network(train_data,train_labels,hi)\n",
        "\n",
        "#print(\"accuracy :-\",Net.accuracy(test_data,test_labels)*100,\"%\")\n",
        "Net.sgd(1,60000,100,0)\n",
        "#print(\"accuracy :-\",Net.accuracy(test_data,test_labels)*100,\"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81ZgkBC9_bGE",
        "outputId": "3031d6d3-f852-4b92-c861-b7d9ae7727e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.41"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "Net.accuracy(test_data,test_labels)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "pUXXVOUXt91X",
        "outputId": "1f538fd0-5ab4-492c-8f64-bfc1c023f905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-f699ff0d0afb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Net.sgd(0.0001,16,100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Net.calculate_accuracy(test_data,test_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, order, like)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ones_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '50' as a data type"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "b =[np.ones(1,50)]\n",
        "\n",
        "# Net.sgd(0.0001,16,100)\n",
        "#Net.calculate_accuracy(test_data,test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ldCCJ-Jxy6"
      },
      "outputs": [],
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "train_data = np.reshape(train_data/255,(len(train_data),train_data.shape[1]**2))\n",
        "test_data = np.reshape(test_data/255,(len(test_data),test_data.shape[1]**2))\n",
        "test_labels =np.reshape(test_labels,(1,len(test_data)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aysDESkXrvaz"
      },
      "outputs": [],
      "source": [
        "print(test_data.shape)\n",
        "print(test_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2nJHtRzKJeg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXu4YpLAiBE8zoI3Shh+b/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}