{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam10816/CS22M082_ASSIGNEMNT_1/blob/main/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhgoo0AzGUZy"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init(project = \"Assignment 1_new\" ,name = \"Question 1\")\n",
        "# fig,axs = plt.subplots(2,5,figsize=(20,6))\n",
        "# axs =axs.flatten()\n",
        "# images=[]\n",
        "# for i in range(0,10):\n",
        "#   index =random.choice(np.where(train_labels==i)[0])\n",
        "  \n",
        "#   axs[i].imshow(train_data[index],cmap=\"gray\")\n",
        "#   axs[i].set_title(titles[i])\n",
        "#   Img = wandb.Image(train_data[index],caption=[titles[i]])\n",
        "#   images.append(Img)\n",
        "# wandb.log({\"examples\":images})\n"
      ],
      "metadata": {
        "id": "CG7IszZv9655"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2/3**"
      ],
      "metadata": {
        "id": "dmMY0qTlUMea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOS7o3xrG1Yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2cccb5e-a77a-4674-f8c1-95d1bd630418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from numpy.core.multiarray import ndarray\n",
        "\n",
        "import wandb\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "#---------------------------import fashion mnist data---------------------------\n",
        "\n",
        "class neural_network:\n",
        "\n",
        "  \n",
        "  #-------------------------Constructor to take train_data and test_data--------\n",
        "  def __init__(self,train_data,train_labels,test_data,test_labels):\n",
        "    \n",
        "    #-----------------------Randomize and Normalize the data----------------------------------\n",
        "\n",
        "    train_data = np.reshape(train_data/255.0,(len(train_data),train_data.shape[1]**2))\n",
        "\n",
        "    # combine train_data and train_label into a single array\n",
        "\n",
        "    train_data_label = np.column_stack((train_data, train_labels))\n",
        "\n",
        "    # shuffle the rows of the combined array in unison\n",
        "    np.random.shuffle(train_data_label)\n",
        "\n",
        "    # separate the shuffled array back into train_data and train_label\n",
        "    train_data = train_data_label[:, :-1]\n",
        "    train_labels = train_data_label[:, -1]\n",
        "\n",
        "    self.test_data = np.reshape(test_data/255.0,(len(test_data),test_data.shape[1]**2))\n",
        "    self.test_labels =test_labels \n",
        "\n",
        "    #-----------------------Split data in train/validation set(90:10)-----------\n",
        "\n",
        "    l=int(train_data.shape[0]/100)*90\n",
        "    self.train_data=train_data[0:l]\n",
        "    self.train_label=train_labels[0:l]\n",
        "    self.validation_data = train_data[l:]\n",
        "    self.validation_label = train_labels[l:]\n",
        "   \n",
        "   \n",
        "    \n",
        "  #------Train function to fit neural network with different parameter----------\n",
        "\n",
        "  def train(self,weight_init=\"random\",hidden_layers=1,size_of_layer=4,activation=\"sigmoid\",optimizer=\"sgd\",learning_rate=0.1,epoch=1,batch_size=4,weight_decay=0.0,loss=\"cross_entropy\",momentum=0.5,beta =0.5,beta1=0.5,beta2=0.5,epsilon=0.000001):\n",
        "    \n",
        "\n",
        "    #-----------------------Weight Initialization-------------------------------\n",
        "    np.random.seed(42)\n",
        "    self.loss = loss\n",
        "    self.hi=[size_of_layer]*hidden_layers\n",
        "    self.activation =activation\n",
        "    if(weight_init==\"Xavier\"):\n",
        "      self.xav()\n",
        "    elif(weight_init==\"random\"):\n",
        "      self.rndm()\n",
        "\n",
        "    #-------------------------------------Optimizer-----------------------------\n",
        "    if(optimizer==\"sgd\"):\n",
        "      self.sgd(step_size=learning_rate,batch_size =batch_size,epoch=epoch,reg=weight_decay)\n",
        "    elif(optimizer==\"momentum\"):\n",
        "      self.mbgd(step_size=learning_rate,batch_size =batch_size,epoch=epoch,beta=momentum,reg=weight_decay)\n",
        "    elif(optimizer==\"nesterov\"):\n",
        "      self.nagd(step_size=learning_rate,batch_size =batch_size,epoch=epoch,beta=momentum,reg=weight_decay)\n",
        "    elif(optimizer==\"rmsprop\"):\n",
        "      self.rmsprop(step_size=learning_rate,batch_size =batch_size,epoch=epoch,beta=beta,reg=weight_decay)\n",
        "    elif(optimizer==\"adam\"):\n",
        "      self.adam(step_size=learning_rate,batch_size =batch_size,epoch=epoch,beta1=beta1,beta2=beta2,reg=weight_decay)\n",
        "    elif(optimizer==\"nadam\"):\n",
        "      self.nadam(step_size=learning_rate,batch_size =batch_size,epoch=epoch,beta1=beta1,beta2=beta2,reg=weight_decay)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "  #------------------------------Xavier Initialization(W & b)-------------------\n",
        "\n",
        "  def xav(self):\n",
        "    l= self.train_data.shape[1]\n",
        "\n",
        "    self.W =[self.xavier_init(self.hi[0],l)] \n",
        "    self.b =[self.xavier_init(1,self.hi[0])]\n",
        "    for i in range(1,len(self.hi)) :\n",
        "      self.W.append(self.xavier_init(self.hi[i],self.hi[i-1]))\n",
        "      self.b.append(self.xavier_init(1,self.hi[i])) \n",
        "    self.W.append(self.xavier_init(10,self.hi[-1]))\n",
        "    \n",
        "    self.b.append(self.xavier_init(1,10))\n",
        "  \n",
        "\n",
        "  #-----------------xavier_init_function(Return Matrix of (n,m))----------------\n",
        "\n",
        "  def xavier_init(self,n, m):\n",
        "    # Calculate the Xavier initialization scale factor\n",
        "    xavier_scale = np.sqrt(2.0 / (n + m))\n",
        "\n",
        "    # Use numpy's random function to generate a matrix of shape (n, m)\n",
        "    matrix = np.random.randn(n, m) * xavier_scale\n",
        "\n",
        "    return matrix\n",
        "\n",
        "  #-----------------------------Random Initialization(W & b)--------------------\n",
        "\n",
        "  def rndm(self):\n",
        "    l= self.train_data.shape[1]\n",
        "\n",
        "    self.W =[np.random.randn(self.hi[0],l)] \n",
        "    self.b =[np.random.randn(1,self.hi[0])]\n",
        "    for i in range(1,len(self.hi)) :\n",
        "      self.W.append(np.random.randn(self.hi[i],self.hi[i-1]))\n",
        "      self.b.append(np.random.randn(1,self.hi[i])) \n",
        "    self.W.append(np.random.randn(10,self.hi[-1]))\n",
        "    \n",
        "    self.b.append(np.random.randn(1,10))\n",
        "\n",
        "  #--------------------------Relu Activation function---------------------------\n",
        "\n",
        "  def relu(self,matrix):\n",
        "    return np.maximum(matrix, 0) \n",
        "\n",
        "  #--------------------------Relu Activation Derivation function---------------\n",
        "\n",
        "  def relu_derivative(self,matrix):\n",
        "    \n",
        "    # Create a copy of the input matrix and convert to float\n",
        "    derivative = np.array(matrix, dtype=np.float64)\n",
        "    \n",
        "    # Set negative values to 0\n",
        "    derivative[derivative < 0] = 0\n",
        "    \n",
        "    # Set positive values to 1\n",
        "    derivative[derivative > 0] = 1\n",
        "\n",
        "    return derivative\n",
        "\n",
        "  #--------------------------Tanh Activation function---------------------------\n",
        "\n",
        "  def tanh(self,matrix):\n",
        "    \n",
        "    # Avoid overflow by scaling inputs to the range [-100, 100]\n",
        "    x = np.clip(matrix, -100, 100)\n",
        "    \n",
        "    # Apply tanh element-wise\n",
        "    return np.tanh(x)\n",
        "\n",
        "  #-------------------------Tanh Activation Derivative function-----------------\n",
        "  def tanh_derivative(self,matrix):\n",
        "   \n",
        "    # Avoid overflow by scaling inputs to the range [-100, 100]\n",
        "    x = np.clip(matrix, -100, 100)\n",
        "    \n",
        "    # Compute tanh element-wise\n",
        "    tanh_x = np.tanh(x)\n",
        "    \n",
        "    # Compute derivative of tanh element-wise\n",
        "    derivative = 1 - tanh_x**2\n",
        "    \n",
        "    \n",
        "    \n",
        "    return derivative\n",
        "\n",
        "  #-------------------------calculates sigmoid for matrix-----------------------\n",
        "\n",
        "  def sigmoid(self,x):\n",
        "  \n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "  #------------------------sigmoid_derivative_function--------------------------\n",
        "\n",
        "  def sigmoid_derivative(self,matrix):\n",
        "   \n",
        "    shift = np.max(matrix, axis=1, keepdims=True)\n",
        "    exp_matrix = np.exp(matrix - shift)\n",
        "    sig = 1 / (1 + exp_matrix)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "  #-------------------------WX_plus_B function----------------------------------\n",
        "\n",
        "  def WX_plus_B(self,W, X, b):\n",
        "    \n",
        "    #matrix multiply\n",
        "    result = np.dot(X, W.transpose())\n",
        "\n",
        "    \n",
        "    #make b of shape result\n",
        "    row_count = result.shape[0]\n",
        "    row_matrix_repeated = np.tile(b, (row_count, 1))\n",
        "\n",
        "    return result + row_matrix_repeated\n",
        "\n",
        "  #----------------------------sum each element of column-----------------------\n",
        "\n",
        "  def sum_columns(self,matrix):\n",
        "    if isinstance(matrix, np.ndarray):\n",
        "        # if matrix is a numpy array, convert it to a list\n",
        "        matrix = matrix.tolist()\n",
        "    \n",
        "    # sum the elements of each column and store in a list\n",
        "    column_sums = [sum(col) for col in zip(*matrix)]\n",
        "    \n",
        "    # convert the list to a 2D matrix of shape (1 x n)\n",
        "    row_matrix = np.array([column_sums])\n",
        "    \n",
        "    return row_matrix\n",
        "\n",
        "  #----------------softmax function for matrix----------------------------------\n",
        "\n",
        "  def softmax(self,x):\n",
        "    \n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "  \n",
        "  #-----------Substract Matrix (W -step_size*W_theta)---------------------------\n",
        "\n",
        "  def subtract_matrices(self,W, W_theta, step_size):\n",
        "    \n",
        "    result_list = []\n",
        "    for i in range(len(self.W)):\n",
        "        result = W[i] - step_size * (W_theta[i])\n",
        "        result_list.append(result)\n",
        "    return result_list\n",
        "\n",
        "  #------------------Mean square error function---------------------------------\n",
        "  def mean_squared_error(self,y_hat, y):\n",
        "    n = y_hat.shape[0]  # number of samples\n",
        "    k = y.astype(int)  # convert y to integer type for indexing\n",
        "    y_k = np.zeros((n, 10))  # create a one-hot encoding of y\n",
        "    y_k[np.arange(n), k] = 1\n",
        "    \n",
        "    # Calculate mean squared error\n",
        "    mse = np.mean((y_hat - y_k)**2)\n",
        "    \n",
        "    return mse\n",
        "  \n",
        "  #-------------------Mean square error Derivative function---------------------\n",
        "\n",
        "  def mean_squared_error_derivative(self,y_hat, y):\n",
        "    n = y_hat.shape[0]  # number of samples\n",
        "    k = y.astype(int)  # convert y to integer type for indexing\n",
        "    y_k = np.zeros((n, 10))  # create a one-hot encoding of y\n",
        "    y_k[np.arange(n), k] = 1\n",
        "    \n",
        "    # Calculate derivative\n",
        "    dMSE_dy_hat = (2/n) * (y_hat - y_k)\n",
        "    \n",
        "    return dMSE_dy_hat\n",
        "  \n",
        "  #--------------------cross_entropy_loss derivative----------------------------\n",
        "\n",
        "  def cross_entropy_loss_derivative(self,y_hat, Y):\n",
        "    ey = np.zeros((y_hat.shape[0],y_hat.shape[1]))\n",
        "\n",
        "    for i in range(0,len(Y)):\n",
        "      ey[i][Y[i]]=1\n",
        "    \n",
        "    return (-(ey-y_hat))\n",
        "\n",
        "  #---------------------Predict labels for Input Data---------------------------\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    A,H,y_hat =self.forward_pro(X_test,self.W,self.b)\n",
        "  \n",
        "    y_pred = np.argmax(y_hat, axis=1)\n",
        "    return y_pred\n",
        "  \n",
        "  #----------------------Forward Propogation Code-------------------------------\n",
        "\n",
        "  def forward_pro(self,X,W,b):\n",
        "\n",
        "    \n",
        "    A=[]\n",
        "    H=[]\n",
        "    A.append(self.WX_plus_B(W[0],X,b[0])) # a0 = WoX +bo\n",
        "    \n",
        "    for i in range(1,len(self.hi)):\n",
        "\n",
        "      H.append(self.activation_fun(A[-1])) # hi = g(ai)\n",
        "     \n",
        "      A.append( self.WX_plus_B(W[i],H[-1],b[i])) # ai = WiX +bi\n",
        "\n",
        "    H.append(self.activation_fun(A[-1]))         # hi  = g(ai)\n",
        "    A.append(self.WX_plus_B(W[-1],H[-1],b[-1]))  # ai = WiX +bi\n",
        "    \n",
        "    #--------apply softmax function for final probbilities----------------------\n",
        "    y_hat = self.softmax((A[-1]))\n",
        "\n",
        "    return A,H,y_hat\n",
        "  \n",
        "  #----------------Back Proppogation function-----------------------------------\n",
        "\n",
        "  def back_prop(self,X,Y,A,H,y_hat):\n",
        "\n",
        "\n",
        "    W_theta , b_theta,H_theta,A_theta =[],[],[],[]\n",
        "    \n",
        "    L =len(A)\n",
        "\n",
        "    #------------------------Check if loss is cross_entropy---------------------\n",
        "    if(self.loss==\"cross_entropy\"):\n",
        "      A_theta.append(self.cross_entropy_loss_derivative(y_hat,Y))\n",
        "    if(self.loss==\"MSE\"):\n",
        "      A_theta.append(self.mean_squared_error_derivative(y_hat,Y))\n",
        "    \n",
        "    #-------------------------W_theta , b_theta calculation---------------------\n",
        "\n",
        "    for k in range(L-1,0,-1):\n",
        "      \n",
        "      W_theta.append((np.matmul(A_theta[-1].transpose(),H[k-1])+self.reg*self.W[k]) ) # athetak*h[k-1]\n",
        "      b_theta.append( self.sum_columns(A_theta[-1]))                                \n",
        "      H_theta.append(np.matmul(A_theta[-1],self.W[k]))\n",
        "  \n",
        "      A_theta.append(H_theta[-1]*self.activation_derivative(A[k-1]))\n",
        "\n",
        "    W_theta.append((np.matmul(A_theta[-1].transpose(),X)+self.reg*self.W[0]))\n",
        "    b_theta.append(self.sum_columns(A_theta[-1]))\n",
        "\n",
        "    W_theta.reverse()\n",
        "    b_theta.reverse()\n",
        "\n",
        "    return W_theta , b_theta\n",
        "\n",
        "  #-------------------------Accuracy Calculation--------------------------------\n",
        "\n",
        "  def accuracy(self, X_test, y_test):\n",
        "    \n",
        "    # Feed forward through the network\n",
        "    A,H,y_hat =self.forward_pro(X_test,self.W,self.b)\n",
        "    \n",
        "    y_pred = np.argmax(y_hat, axis=1)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    acc = np.mean(y_pred == y_test)\n",
        "    # Calculate accuracy\n",
        "  \n",
        "    return acc\n",
        "\n",
        "  #---------------------------Cross Entropy Function----------------------------\n",
        "  def cross_entropy(self,y_hat,Y):\n",
        "\n",
        "      epsilon = 1e-9\n",
        "      y_hat = np.maximum(y_hat, epsilon) # to avoid invalid divide\n",
        "\n",
        "      sum=0.0;\n",
        "      for i in range(0,len(Y)):\n",
        "        sum+=(-np.log2(y_hat[i][Y[i]]))\n",
        "      sum/= float(len(Y))\n",
        "\n",
        "      return sum\n",
        "  \n",
        "  #-------------------------Stochastic Gradient Descent-------------------------\n",
        "\n",
        "  def sgd(self,step_size,batch_size,epoch,reg=0.9):\n",
        "\n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    rate = step_size\n",
        "\n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)        # Learning Rate Updation\n",
        "      \n",
        "      for k in range(0,N):\n",
        "\n",
        "          # -------------------Create  Batch of particular size-----------------\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #---------------Compute A,H,y_hat-------------------------------------\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "\n",
        "          #---------------Compute dW ,dw using back_propogation-----------------\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          #-------------------------Update W and b------------------------------\n",
        "          self.b =self.subtract_matrices(self.b,db,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,dW,step_size)\n",
        "          \n",
        "      # Calculate validation Loss,validation Accuracy , Training_loss , Training Accuracy    \n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      #-------Update values To Wandb------\n",
        "\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "\n",
        "  #-------------------------Momentum based gradient descent---------------------\n",
        "\n",
        "  def mbgd(self,step_size,batch_size,epoch,beta=0.9,reg=0.005):\n",
        "\n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "\n",
        "    prev_ub , prev_uw =[],[]\n",
        "\n",
        "    #---------------Initilalize All matrix with 0-------------------------------\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      prev_ub.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      prev_uw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "    \n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)\n",
        "      start_time = time.time()\n",
        "      beta_t=beta\n",
        "      for k in range(0,N):\n",
        "          ub,uw = list(prev_ub),list(prev_uw)\n",
        "\n",
        "          #------------------------Create Minibatch data------------------------\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #--------calculate dW ,db using forward and backword propogation------\n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          #-----------------------weight update---------------------------------\n",
        "          for i in range(len(self.W)):\n",
        "            ub[i]= beta_t*prev_ub[i] + db[i]\n",
        "            uw[i]= beta_t*prev_uw[i] + dW[i]\n",
        "          self.b =self.subtract_matrices(self.b,ub,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,uw,step_size)\n",
        "\n",
        "          prev_ub , prev_uw = list(ub),list(uw)\n",
        "\n",
        "      # Calculate val_accuracy ,val_loss ,training accuracy ,training loss\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      #---------Calculate perticular loss----------------\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      #------------------------Update to Wandb----------------------------------\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "\n",
        "  #-------------------Nesterov Accelerated gradient descent---------------------    \n",
        "  def nagd(self,step_size,batch_size,epoch,beta=0.9,reg=0.005):\n",
        "\n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "\n",
        "    #---------------Initilalize All matrix with 0-------------------------------\n",
        "\n",
        "    prev_ub , prev_uw =[],[]\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      prev_ub.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      prev_uw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "\n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "      beta_t=beta\n",
        "      step_size=rate/(e+1)           #learning rate updation\n",
        "      start_time = time.time()\n",
        "      for k in range(0,N):\n",
        "          ub,uw = list(prev_ub),list(prev_uw)\n",
        "          n_w ,n_b =self.subtract_matrices(self.W,prev_uw,beta_t),self.subtract_matrices(self.b,prev_ub,beta_t)\n",
        "\n",
        "          #------------------------Create Minibatch data------------------------\n",
        "\n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #--------calculate dW ,db using forward and backword propogation------\n",
        "          A,H,y_hat=self.forward_pro(minibatch,n_w,n_b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          #-----------------------weight update---------------------------------\n",
        "\n",
        "          for i in range(len(self.W)):\n",
        "            ub[i]= beta_t*prev_ub[i] + db[i]\n",
        "            uw[i]= beta_t*prev_uw[i] + dW[i]\n",
        "          self.b =self.subtract_matrices(self.b,ub,step_size)\n",
        "          self.W =self.subtract_matrices(self.W,uw,step_size)\n",
        "\n",
        "          prev_ub , prev_uw = list(ub),list(uw)\n",
        "      \n",
        "      # Calculate val_accuracy ,val_loss ,training accuracy ,training loss\n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      #------------------------Update to Wandb----------------------------------\n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "\n",
        "  def rmsprop(self,step_size,batch_size,epoch,beta=0.9,reg=0.005,epsilon=1e-10):\n",
        "    \n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "\n",
        "    ub , uw =[],[]\n",
        "\n",
        "    #---------------Initilalize All matrix with 0-------------------------------\n",
        "    for i in range(len(self.W)):\n",
        "      ub.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      uw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "\n",
        "    rate = step_size\n",
        "    for e in range(0,epoch):\n",
        "\n",
        "      # Learning Rate Update\n",
        "      step_size=rate/(e+1)      \n",
        "      \n",
        "      beta_t = beta\n",
        "      for k in range(0,N):\n",
        "         \n",
        "          #------------------------Create Minibatch data------------------------\n",
        "          \n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #--------calculate dW ,db using forward and backword propogation------\n",
        "          \n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          #-----------------------weight update---------------------------------\n",
        "          \n",
        "          for i in range(len(self.W)):\n",
        "            ub[i]= beta_t*ub[i] + (1-beta_t)*(db[i]**2)\n",
        "            uw[i]= beta_t*uw[i] + (1-beta_t)*(dW[i]**2)\n",
        "          \n",
        "          for i in range(len(self.W)):\n",
        "            result_b = self.b[i] - step_size*db[i]/(np.sqrt(ub[i])+epsilon)\n",
        "            result_w = self.W[i] - step_size*dW[i]/(np.sqrt(uw[i])+epsilon)\n",
        "            self.b[i]=result_b\n",
        "            self.W[i]=result_w\n",
        "\n",
        "      #Calculate val_accuracy ,val_loss ,training accuracy ,training loss\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      #---------Calculate perticular loss---------------------------------------\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "      \n",
        "      #------------------------Update to Wandb----------------------------------\n",
        "      \n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "      \n",
        "  \n",
        "  def adam(self,step_size,batch_size,epoch,beta1=0.9,beta2=0.999,reg=0.005,epsilon=1e-4):\n",
        "\n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    rate = step_size\n",
        "    vb , vw =[],[]\n",
        "\n",
        "    #---------------Initilalize All matrix with 0-------------------------------\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      vb.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      vw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "    mw=list(vw)\n",
        "    mb=list(vb)\n",
        "\n",
        "    \n",
        "    for e in range(0,epoch):\n",
        "\n",
        "      # update learning rate\n",
        "      step_size=rate/(e+1)            \n",
        "      beta1_t,beta2_t = beta1,beta2\n",
        "      \n",
        "      for k in range(0,N):\n",
        "         \n",
        "          #------------------------Create Minibatch data------------------------\n",
        "          \n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #--------calculate dW ,db using forward and backword propogation------\n",
        "          \n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "\n",
        "          #-----------------------weight update---------------------------------\n",
        "          \n",
        "          for i in range(len(self.W)):\n",
        "            mw[i]=  beta1_t*mw[i] + (1-beta1_t)*dW[i]\n",
        "            mb[i]=  beta1_t*mb[i] + (1-beta1_t)*db[i]\n",
        "            vb[i]= beta2_t*vb[i] + (1-beta2_t)*db[i]**2\n",
        "            vw[i]= beta2_t*vw[i] + (1-beta2_t)*dW[i]**2\n",
        "\n",
        "            mw_hat=mw[i]/(1-np.power(beta1_t,e+1))\n",
        "            mb_hat=mb[i]/(1-np.power(beta1_t,e+1))\n",
        "            vw_hat=vw[i]/(1-np.power(beta2_t,e+1))\n",
        "            vb_hat=vb[i]/(1-np.power(beta2_t,e+1))\n",
        "          \n",
        "            result_b = self.b[i] - step_size*mb_hat/(np.sqrt(vb_hat)+epsilon)\n",
        "            result_w = self.W[i] - step_size*mw_hat/(np.sqrt(vw_hat)+epsilon)\n",
        "            self.b[i]=result_b\n",
        "            self.W[i]=result_w\n",
        "\n",
        "      # Calculate val_accuracy ,val_loss ,training accuracy ,training loss\n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      #------------------------Update values to Wandb----------------------------------\n",
        "      \n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "  \n",
        "  def nadam(self,step_size,batch_size,epoch,beta1=0.9,beta2=0.999,reg=0.005,epsilon=1e-4):\n",
        "\n",
        "    #-----------------------Number of batches-----------------------------------\n",
        "\n",
        "    N = int(len(self.train_data)/batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.reg =reg\n",
        "    rate = step_size\n",
        "    vb , vw =[],[]\n",
        "\n",
        "    #---------------Initilalize vb ,vw matrix with 0-------------------------------\n",
        "\n",
        "    for i in range(len(self.W)):\n",
        "      vb.append(np.zeros((self.b[i].shape[0],self.b[i].shape[1])))\n",
        "      vw.append(np.zeros((self.W[i].shape[0],self.W[i].shape[1])))\n",
        "\n",
        "    mw=list(vw)\n",
        "    mb=list(vb)\n",
        "\n",
        "    \n",
        "    for e in range(0,epoch):\n",
        "      step_size=rate/(e+1)            # update learning rate\n",
        "\n",
        "      beta1_t,beta2_t = beta1,beta2\n",
        "      \n",
        "      for k in range(0,N):\n",
        "         \n",
        "          #------------------------Create Minibatch data------------------------\n",
        "          \n",
        "          minibatch = self.train_data[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "          minibatch_lable=self.train_label[k*batch_size:min(k*batch_size+batch_size,len(self.train_data))]\n",
        "\n",
        "          #--------calculate dW ,db using forward and backword propogation------\n",
        "          \n",
        "          A,H,y_hat=self.forward_pro(minibatch,self.W,self.b)\n",
        "          dW,db = self.back_prop(minibatch,minibatch_lable,A,H,y_hat)\n",
        "          \n",
        "          #-----------------------weight update---------------------------------\n",
        "          \n",
        "          for i in range(len(self.W)):\n",
        "            mw[i]=  beta1_t*mw[i] + (1-beta1_t)*dW[i]\n",
        "            mb[i]=  beta1_t*mb[i] + (1-beta1_t)*db[i]\n",
        "            vb[i]= beta2_t*vb[i] + (1-beta2_t)*db[i]**2\n",
        "            vw[i]= beta2_t*vw[i] + (1-beta2_t)*dW[i]**2\n",
        "\n",
        "            mw_hat=mw[i]/(1-np.power(beta1_t,e+1))\n",
        "            mb_hat=mb[i]/(1-np.power(beta1_t,e+1))\n",
        "            vw_hat=vw[i]/(1-np.power(beta2_t,e+1))\n",
        "            vb_hat=vb[i]/(1-np.power(beta2_t,e+1))\n",
        "          \n",
        "            result_w = self.W[i] -(step_size/np.sqrt(vw_hat+epsilon))*(beta1_t*mw_hat+(1-beta1_t)*dW[i]/(1-beta1_t**(e+1)))\n",
        "            result_b = self.b[i] -(step_size/np.sqrt(vb_hat+epsilon))*(beta1_t*mb_hat+(1-beta1_t)*db[i]/(1-beta1_t**(e+1)))\n",
        "            self.b[i]=result_b\n",
        "            self.W[i]=result_w\n",
        "      \n",
        "      # Calculate val_accuracy ,val_loss ,training accuracy ,training loss\n",
        "\n",
        "      A,H,y_hat=self.forward_pro(self.validation_data,self.W,self.b)\n",
        "      A,H,train_hat=self.forward_pro(self.train_data,self.W,self.b)\n",
        "\n",
        "      validation_loss = 0\n",
        "      train_loss=0\n",
        "\n",
        "      if(self.loss==\"cross_entropy\"):\n",
        "        validation_loss=self.cross_entropy(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "      if(self.loss==\"MSE\"):\n",
        "        validation_loss=self.mean_squared_error(y_hat,self.validation_label)\n",
        "        train_loss= self.cross_entropy(train_hat,self.train_label)\n",
        "\n",
        "      train_acc = self.accuracy(self.validation_data,self.validation_label)\n",
        "      validation_accuracy =self.accuracy(self.validation_data,self.validation_label)\n",
        "\n",
        "      #------------------------Update to Wandb----------------------------------\n",
        "      \n",
        "      wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "      wandb.log({\"validation_loss\": validation_loss})  \n",
        "      wandb.log({\"train_accuracy\":train_acc})\n",
        "      wandb.log({\"training_loss\":train_loss})\n",
        "\n",
        "  # compute matrix and return result according to activation parameter\n",
        "  def activation_fun(self,matrix):\n",
        "\n",
        "      if(self.activation==\"sigmoid\"):\n",
        "        return self.sigmoid(matrix)\n",
        "\n",
        "      elif(self.activation==\"tanh\"):\n",
        "        return self.tanh(matrix)\n",
        "\n",
        "      elif(self.activation==\"ReLU\"):\n",
        "        return self.relu(matrix)\n",
        "  \n",
        "  # compute matrix and return result according to activation parameter\n",
        "  def activation_derivative(self,matrix):\n",
        "\n",
        "      if(self.activation==\"sigmoid\"):\n",
        "        return self.sigmoid_derivative(matrix)\n",
        "\n",
        "      elif(self.activation==\"tanh\"):\n",
        "        return self.tanh_derivative(matrix)\n",
        "\n",
        "      elif(self.activation==\"ReLU\"):\n",
        "        return self.relu_derivative(matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question.4**"
      ],
      "metadata": {
        "id": "57JWtdByTpyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This is configuration for Wandb where we have separate function named train_nn\n",
        "# Which will train  neural network for different swwep id with specific configuration\n",
        "# choosen by Wandb.\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    \n",
        "\n",
        "    \"method\": 'bayes',\n",
        "    \"metric\":{\n",
        "        'name':'accuracy',\n",
        "        'goal':'maximize'\n",
        "    },\n",
        "    'parameters' :{\n",
        "        \"weight_init\" :{\"values\":[\"random\",\"Xavier\"]},\n",
        "        \"hidden_layers\": {\"values\": [ 3,4,5,6]},\n",
        "        \"size_of_layer\": {\"values\": [ 32, 64,128]},\n",
        "        \"activation\": {\"values\": [\"sigmoid\", \"ReLU\",\"tanh\"]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\",\"momentum\",\"nesterov\", \"adam\",\"rmsprop\",\"nadam\"]},\n",
        "        \"learning_rate\": {\"values\": [0.001,0.0001,0.00001]},\n",
        "        \"epoch\": {\"values\": [5,10]},\n",
        "        \"batch_size\": {\"values\": [16,32,64]},\n",
        "        \"weight_decay\": {\"values\": [0.0005, 0.005, 0.05]}\n",
        "    }\n",
        "    \n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def train_nn():\n",
        "\n",
        "    config_default={\n",
        "    'weight_init':\"random\",\n",
        "    'hidden_layers':3,\n",
        "    'size_of_layer':32,\n",
        "    'activation':\"sigmoid\",\n",
        "    'optimizer':\"sgd\",\n",
        "    'learning_rate':0.01,\n",
        "    'epoch':10,\n",
        "    'batch_size':32,\n",
        "    'weight_decay':0\n",
        "    }\n",
        "    \n",
        "   \n",
        "    wandb.init(config=config_default)\n",
        "    config = wandb.config\n",
        "\n",
        "    # perticular name of sweep\n",
        "    name='init_'+str(config.weight_init)+'_hl_'+str(config.hidden_layers)+\"_SL_\"+str(config.size_of_layer)+'_BS_'+str(config.batch_size)+\"_LR_\"+str(config.learning_rate)+'_AF_'+str(config.activation)+'_OPT_'+str(config.optimizer)+'_epoch_'+str(config.epoch)\n",
        "    wandb.init(name=name)\n",
        "\n",
        "    # train Neural Network\n",
        "    Net = neural_network(train_data,train_labels,test_data,test_labels)\n",
        "    Net.train(epoch=config.epoch, hidden_layers=config.hidden_layers, size_of_layer=config.size_of_layer, batch_size=config.batch_size, activation=config.activation, optimizer=config.optimizer, weight_init=config.weight_init, learning_rate=config.learning_rate, weight_decay=config.weight_decay,loss=\"MSE\")\n",
        "    \n",
        "    \n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"Assignment1(MSE+Bayes)\")\n",
        "wandb.agent(sweep_id, function = train_nn)\n"
      ],
      "metadata": {
        "id": "HD8pkJeDAW5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will train Neural Network with best of configuration and\n",
        "# give ouput y_hat for test_data\n",
        "\n",
        "# Define best configuration from experiments\n",
        "config_default={\n",
        "    'weight_init':\"Xavier\",\n",
        "    'hidden_layers':5,\n",
        "    'size_of_layer':128,\n",
        "    'activation':\"tanh\",\n",
        "    'optimizer':\"nadam\",\n",
        "    'learning_rate':0.001,\n",
        "    'epoch':10,\n",
        "    'batch_size':16,\n",
        "    'weight_decay':0.005\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "wandb.init(config=config_default)\n",
        "config = wandb.config\n",
        "name='init_'+str(config.weight_init)+'_hl_'+str(config.hidden_layers)+\"_SL_\"+str(config.size_of_layer)+'_BS_'+str(config.batch_size)+\"_LR_\"+str(config.learning_rate)+'_AF_'+str(config.activation)+'_OPT_'+str(config.optimizer)+'_epoch_'+str(config.epoch)\n",
        "Net = neural_network(train_data,train_labels,test_data,test_labels)\n",
        "\n",
        "# Fit the model with perticular configuration\n",
        "wandb.init(project = \"Best_sweep\" ,name =name) \n",
        "Net.train(epoch=config.epoch, hidden_layers=config.hidden_layers, size_of_layer=config.size_of_layer, batch_size=config.batch_size, activation=config.activation, optimizer=config.optimizer, weight_init=config.weight_init, learning_rate=config.learning_rate, weight_decay=config.weight_decay)\n",
        "\n",
        "# generate labels for test_data\n",
        "y_pred=Net.predict(Net.test_data)  \n",
        "\n",
        "# get true labels of test data\n",
        "y_true = Net.test_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "pQmgOmrtdXSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "\n",
        "# This code will plot Confusion matrix\n",
        "\n",
        "# Define the confusion matrix and class labels\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "classes = [\"T-Shirt/Top\",\"Trouser\",\"Pullover\",\"Dress\",\"Shirts\",\"Sandal\",\"Coat\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the percentages\n",
        "percentages = (cm / np.sum(cm)) * 100\n",
        "\n",
        "# Define the text for each cell\n",
        "cell_text = []\n",
        "for i in range(len(classes)):\n",
        "    row_text = []\n",
        "    for j in range(len(classes)):\n",
        "\n",
        "        txt = \"Total \"+f'{cm[i, j]}<br>Per. ({percentages[i, j]:.3f})'\n",
        "        if(i==j):\n",
        "          txt =\"Correcty Predicted \" +classes[i]+\"<br>\"+txt\n",
        "        if(i!=j):\n",
        "          txt =\"Predicted \" +classes[j]+\" For \"+classes[i]+\"<br>\"+txt\n",
        "        row_text.append(txt)\n",
        "    cell_text.append(row_text)\n",
        "\n",
        "# Define the trace\n",
        "trace = go.Heatmap(z=percentages,\n",
        "                   x=classes,\n",
        "                   y=classes,\n",
        "                   colorscale='Blues',\n",
        "                   colorbar=dict(title='Percentage'),\n",
        "                   hovertemplate='%{text}%<extra></extra>',\n",
        "                   text=cell_text,\n",
        "                   )\n",
        "\n",
        "# Define the layout\n",
        "layout = go.Layout(title='Confusion Matrix',\n",
        "                   xaxis=dict(title='Predicted Classes'),\n",
        "                   yaxis=dict(title='True Classes'),\n",
        "                   )\n",
        "\n",
        "# Plot the figure\n",
        "fig = go.Figure(data=[trace], layout=layout)\n",
        "wandb.log({'confusion_matrix': (fig)})"
      ],
      "metadata": {
        "id": "B2-RCyRJJDnP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNelkth7hRYaXk4RtloQaBE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}